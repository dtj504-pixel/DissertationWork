<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Emma Bowen">

<title>Explanation of Mathematical Methods Behind the First Half of the Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Draft_of_Project_due_Week_5_files/libs/clipboard/clipboard.min.js"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/popper.min.js"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Draft_of_Project_due_Week_5_files/libs/quarto-html/anchor.min.js"></script>
<link href="Draft_of_Project_due_Week_5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Draft_of_Project_due_Week_5_files/libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Draft_of_Project_due_Week_5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Draft_of_Project_due_Week_5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Draft_of_Project_due_Week_5_files/libs/bootstrap/bootstrap-55aa396ea3e988129bcec43acf404917.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Explanation of Mathematical Methods Behind the First Half of the Project</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Emma Bowen </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="contents" class="level1">
<h1>Contents</h1>
</section>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>Sustainable fisheries management requires a delicate balance of maximising long-term catch whilst keeping the risk of stock depletion below precautionary thresholds. Traditionally, Management Strategy Evaluation (MSE) grid searches are used to find optimal Harvest Control Rules (HCRs). However, simulating these complex, interconnected dynamics is computationally expensive and time-consuming, particularly as management shifts from single-stock assessments to more realistic, multispecies mixed fisheries.</p>
<p>This project introduces a computationally efficient framework for evaluating HCRs. Initially applied to a single-stock fishery, the framework utilises Gaussian process regression to independently model both catch and risk. Then, by combining the methods of Bayesian history matching, acquisition functions and k-means clustering, we are able to systematically map the sample space. This drastically reduces the need for expensive simulation runs.</p>
<p>Building on this foundation, we scale up this framework to complex mixed-fisheries scenarios. Here, the objective becomes identifying the fishing mortality for each of the interacting stocks that safely maximizes total long-term catch. By modelling long-term catch, the framework inherently minimizes the risk of stock crashes and this is reinforced by a risk calculation that follows the ICES precautionary standard.</p>
<p>This framework provides a powerful tool for marine policy by significantly reducing the computational burden of mixed-fishery MSEs. This method delivers timely, evidence-based MSEs that ensures that the stocks remain both economically viable and firmly within safe biological limits.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>NO CURRENT APPROPRIATE ONE IN SOURCE MATERIAL</p>
</section>
<section id="creating-the-framework-in-a-single-stock-fisheries-context" class="level1">
<h1>Creating the framework in a single stock fisheries context</h1>
<p>LIST BELOW LIKELY TO BE MADE OBVIOUS BY CONTENTS TABLE</p>
<p>What mathematical methods will I explain in this section?</p>
<p>Gaussian processes</p>
<p>Bayesian History Matching</p>
<p>Kriging</p>
<p>Expected Improvement</p>
<p>Augmented Expected Improvement</p>
<p>Knowledge Gradient</p>
<p>K-Means Clustering</p>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>We are building on the paper ``Using history matching to speed up management strategy evaluation grid searches’’. This paper is looking to find the Harvest Control Rule parametrised by <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that maximises the median long-term catch whilst keeping the risk below 0.05 for a single-stock fishery. The paper does not consider fleet dynamics. We have an objective function in the paper which combines the risk and the catch and so this is the function we want to maximise to get our maximal median catch with the constraint of keeping the risk below 0.05.</p>
<p>To do this, the paper takes a Bayesian History Matching (BHM) approach. Firstly, we sample our first eight points which are spaced evenly throughout the sample space to get some initial data. Then, for each round we do the following:</p>
<ul>
<li><p>We set up or update the Gaussian Process (GP) to model the risk and the GP to model the median catch</p></li>
<li><p>We can then use the risk as a threshold so that we only consider the values of <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that have risk below 0.05</p></li>
<li><p>We use the GP which is modelling the median catch to get the value for the median catch at every point in the sample space, which will have some uncertainty</p></li>
<li><p>We use BHM to remove any points that are implausible (that have a low probability of being higher than the current best median catch)</p></li>
<li><p>We select 8 plausible points to sample in the next round</p></li>
</ul>
<p>Now, we will look at the mathematics behind the methods above and also at the acquisition functions of Expected Improvement, Augmented Expected Improvement and Knowledge Gradient which I added to the original code.</p>
</section>
<section id="general-theory" class="level2">
<h2 class="anchored" data-anchor-id="general-theory">General theory</h2>
<section id="gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-processes">Gaussian Processes</h3>
<p>A Gaussian Process <span class="math inline">\(F\)</span> has a mean function <span class="math inline">\(\mu_0\)</span> and a covariance function <span class="math inline">\(\operatorname{cov}_0(x_i,x_j)\)</span>. We can then evaluate the covariance function <span class="math inline">\(\operatorname{cov}_0(x_i,x_j)\)</span> for every pair <span class="math inline">\(x_i,x_j\)</span> where <span class="math inline">\(i,j\in\{1, ..., n\}\)</span> to find the covariance matrix <span class="math inline">\(\Sigma_{1:n}\)</span> . Then, <span class="math inline">\(F\)</span> is a probability distribution over our objective function <span class="math inline">\(f\)</span> with the property that, for any given collection of points <span class="math inline">\({x_1,...x_n}\)</span>, the marginal probability distribution on <span class="math inline">\(F(x_{1:n}) = (F(x_1),...,F(x_n))\)</span> is given by <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span class="math display">\[
F(x_{1:n}) \sim N(\mu_0(x_{1:n}),\Sigma_{1:n})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mu_0(x_{1:n}) = (\mu_0(x_1), . . ., \mu_0(x_n))
\]</span></p>
<p>We choose a covariance function such that the inputs that have nearby points that have been evaluated have a more certain output than points that are further away from the points that have been evaluated <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This is equivalent to saying that if for some <span class="math inline">\(x,x',x''\)</span> in the design space we have <span class="math inline">\(\|x - x'\| &lt; \|x -x''\|\)</span> for some norm <span class="math inline">\(\| \cdot \|\)</span>, then <span class="math inline">\(\operatorname{cov}_0(x, x') &gt; \operatorname{cov}_0(x, x'')\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>We use a GP to emulate the objective function because it is much cheaper to evaluate than our objective function. We can calculate <span class="math inline">\(F(x)\)</span> for any <span class="math inline">\(x\)</span> in the design space as our estimate of <span class="math inline">\(f(x)\)</span> based on our current beliefs. This is true even for the evaluated points <span class="math inline">\(x_1,...,x_m\)</span> as the emulator is fitted to these points <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
</section>
<section id="maximum-likelihood-estimation" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>When using GPs to emulate our objective function, we need to be able to estimate the hyperparameters of the GP using the data we gain from evaluating our points <span class="math inline">\(x_1,...,x_n\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>We can do this as follows. Firstly, we let the vector <span class="math inline">\(\eta\)</span> represent the hyperparameters that give us <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\operatorname{cov}_0\)</span>. Then, given the observations <span class="math inline">\(f(x_{1:n}) = (f(x_1),...,f(x_n))\)</span>, we calculate the likelihood of these observations under the prior given <span class="math inline">\(\eta\)</span> which is denoted as <span class="math inline">\(p(f(x_{1:n})|\eta)\)</span> and modelled by <strong>?@eq-MultivariateNormalDist</strong> . Lastly, we set <span class="math inline">\(\hat{\eta}\)</span> to the value that maximizes this likelihood <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span class="math display">\[
\hat{\eta} = argmax_\eta p(f(x_{1:n})|\eta)
\]</span></p>
</section>
<section id="kriging" class="level3">
<h3 class="anchored" data-anchor-id="kriging">Kriging</h3>
<p>Kriging is a Bayesian statistical method for modelling functions <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. Again, let <span class="math inline">\(f\)</span> be the objective function and we focus on the design space <span class="math inline">\(X := \{x_1,...,x_k\}\)</span>. Now, if we have evaluated <span class="math inline">\(n\)</span> points such that we have <span class="math inline">\(f(x_{1:n})\)</span> and want to evaluate <span class="math inline">\(x_{n+1}\)</span> we let <span class="math inline">\(k = n+1\)</span> in <strong>?@eq-MultivariateNormalDist</strong> . Then, we can compute the conditional distribution of <span class="math inline">\(F(x_{n+1})\)</span> given <span class="math inline">\(f(x_{1:n})\)</span> using Bayes’ rule:</p>
<p><span class="math display">\[
F(x_{n+1})|f(x_{1:n}) \sim N(\mu_n(x_{n+1}), \sigma^2_n(x_{n+1}))
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mu_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}(f(x_{1:n}) - \mu_0(x_{1:n}))  + \mu_0(x_{n+1})
\]</span></p>
<p><span class="math display">\[
\sigma^2_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1},x_{n+1}) - \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}\operatorname{cov}_0(x_{1:n},x_{n+1})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\operatorname{cov}_0(x_{n+1}, x_{1:n}) = (\operatorname{cov}_0(x_{n+1}, x_1), ... , \operatorname{cov}_0(x_{n+1}, x_n))
\]</span></p>
<p>This conditional distribution <span class="math inline">\(F(x_{n+1})|f(x_{1:n})\)</span> is called the posterior probability distribution for <span class="math inline">\(x_{n+1}\)</span>. We can calculate this distribution for every point in the design space <span class="math inline">\(X\)</span>. This results in a new GP <span class="math inline">\(F_n\)</span> with a mean vector and covariance kernel that depend on the location of the unevaluated points, the locations of the evaluated points <span class="math inline">\(x_{1:n}\)</span>, and their values <span class="math inline">\(f(x_{1:n})\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. So, we can update our GP every round based on the new points we have evaluated.</p>
</section>
<section id="bayesian-history-matching" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-history-matching">Bayesian History Matching</h3>
<p>Let <span class="math inline">\(x\)</span> be a point in the sample space. We begin with some uncertainty about our objective function <span class="math inline">\(f(x)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. However, we can make probabilistic statements such as:</p>
<p><span class="math display">\[\begin{equation}
    P(f(x)&gt;a)= \int_{a}^{\infty}P(f(x))df(x)
\end{equation}\]</span></p>
<p>Once we evaluate another point <span class="math inline">\(x'\)</span> where <span class="math inline">\(x' \neq x\)</span>, we are able to use Bayes’ Theorem improve our integral to:</p>
<p><span class="math display">\[\begin{equation}
    P(f(x)&gt;a|f(x'))= \int_{a}^{\infty}P(f(x)|f(x'))df(x)
\end{equation}\]</span></p>
<p>We now let <span class="math inline">\(a=max\{f(x_1),...,f(x_n)\}\)</span> where <span class="math inline">\(n\)</span> is the number of points we have evaluated so far. For the first round, <span class="math inline">\(n=8\)</span> but as the rounds increase we make sure to include all previous points of the objective function that have been evaluated <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. We remove the point <span class="math inline">\(x\)</span> if:</p>
<p><span class="math display">\[
P(f(x)&gt;a|f(x_{1:n})) = \int_{a}^{\infty}P(f(x)|f(x_{1:n}))df(x) &lt;\varepsilon
\]</span></p>
<p>for a small <span class="math inline">\(\varepsilon &gt; 0\)</span> until no plausible points remain. Then, the optimum will be <span class="math inline">\(x^*\)</span> such that:</p>
<p><span class="math display">\[
f(x^*) = max\{f(x_{1:n})\}
\]</span></p>
<p>as our index <span class="math inline">\(n\)</span> counts the number of points we have evaluated throughout the whole simulation.</p>
</section>
<section id="expected-improvement" class="level3">
<h3 class="anchored" data-anchor-id="expected-improvement">Expected Improvement</h3>
<p>The first type of acquisition function we will look at is Expected Improvement (EI). Suppose we have sampled the points <span class="math inline">\(x_1, ... ,x_n\)</span> and observe the values <span class="math inline">\(f(x_{1:n})\)</span>. Then, if we were to return a solution at this point, bearing in mind we observe the objective function <span class="math inline">\(f\)</span> without noise and we can only return points we have already evaluated, we would return <span class="math inline">\(f^*_n = max\{f(x_1),...,f(x_n)\}\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. Imagine we then consider evaluating another point <span class="math inline">\(x_{n+1}\)</span> to get <span class="math inline">\(f(x_{n+1})\)</span>. We can then define the Expected Improvement as:</p>
<p><span class="math display">\[\begin{equation}
    EI_n(x_{n+1}) := E[F(x_{n+1})|f(x_{1:n})-f^*_n]^+
\end{equation}\]</span></p>
<p>where <span class="math inline">\([F(x_{n+1})-f^*_n]^+\)</span> is the positive part of <span class="math inline">\([F(x_{n+1})-f^*_n]\)</span>. This acquisition function is relatively easy to optimise and many different methods have been developed for doing this <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>There is another expression for <span class="math inline">\(EI_n(x_{n+1})\)</span>:</p>
<p><span class="math display">\[
EI_n(x_{n+1}) =  [(\mu_n(x_{n+1})-f_n^*\cdot\Phi(Z))+(\sigma_n(x_{n+1})\cdot\phi(Z))]^+
\]</span></p>
<p>where again the notation <span class="math inline">\([\cdot]^+\)</span> means the positive part and where:</p>
<p><span class="math display">\[
Z = \frac{\mu_n(x_{n+1})-f_n^*}{\sigma_n(x_{n+1})}
\]</span></p>
<p><strong>?@eq-EIincode</strong> can be gained from <strong>?@eq-MultivariateNormalDist</strong> by setting <span class="math inline">\(k = n+1\)</span> and then studying the distribution of <span class="math inline">\(F(x_{n+1})-f^*_n\)</span>. However, we can also consider it as a version of Equation (15) from <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">Jones, Schonlau, and Welch (<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">1998</a>)</span> where we first flip the signs as we are focused on the maximisation case and then set <span class="math inline">\(f_{min} = f^*_n\)</span>, <span class="math inline">\(\hat{y} = \mu_n(x)\)</span> and <span class="math inline">\(s = \sigma_n(x)\)</span>.</p>
</section>
<section id="augmented-expected-improvement" class="level3">
<h3 class="anchored" data-anchor-id="augmented-expected-improvement">Augmented Expected Improvement</h3>
<p>This was included to help make the method perform better for noisy functions which will make it more generally applicable <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>. To deal with these noisy observations, a change was proposed to the standard EI function as detailed below. This change seems mostly to have been justified by empirical performance <span class="citation" data-cites="letham2018constrainedbayesianoptimizationnoisy">(<a href="#ref-letham2018constrainedbayesianoptimizationnoisy" role="doc-biblioref">Letham et al. 2018</a>)</span>.</p>
<p>By adjusting Equation (12) found in <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">Huang et al. (<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">2006</a>)</span> to our own notation, we get that:</p>
<p><span class="math display">\[\begin{equation}
    AEI_n(x_{n+1})=E[F(x_{n+1})|f(x_{1:n})-f^*_{eb}]^+\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma_{obs}\)</span> is the standard deviation of the noise variable set by the user and <span class="math inline">\(\sigma_n(x)\)</span> is the standard deviation of GP <span class="math inline">\(F\)</span> at the <span class="math inline">\(n^{th}\)</span> iteration, as used beforehand. We have also changed <span class="math inline">\(f^*_n\)</span> to <span class="math inline">\(f^*_{eb}\)</span> which is the highest predicted mean at any sampled point so far so that we take into account that the uncertainty in our observations could cause a large spike <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
</section>
<section id="knowledge-gradient" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-gradient">Knowledge Gradient</h3>
<p>We remove the assumption of EI that we have to return a pre-evaluated point as our best point <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. This allows us to do some different computations to the ones in EI. We also now start by saying that the solution we would choose if we have to stop sampling after <span class="math inline">\(n\)</span> points would be the point in the design space with the largest <span class="math inline">\(\mu_n(\cdot)\)</span> value, where <span class="math inline">\(\mu_n(\cdot)\)</span> is the mean vector of the posterior probability distribution after <span class="math inline">\(n\)</span> iterations. We call this maximum <span class="math inline">\({x_n^*}\)</span> and then can say that <span class="math inline">\(F(x_n^*)\)</span> is random under the posterior distribution and has the mean vector after sampling <span class="math inline">\(f(x_{1:n})\)</span> of:</p>
<p><span class="math display">\[
\mu_n^* := \mu_n(x_n^*) = max_{x}\mu_n(x)
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is any point in the sample space <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>Then, we imagine that we are now allowed to sample a new point <span class="math inline">\(x_{n+1}\)</span>. We get a new posterior distribution at the point <span class="math inline">\(x\)</span> which we can calculate using <strong>?@eq-posteriordistributiongivensamples</strong> by replacing <span class="math inline">\(x_{n+1}\)</span> with <span class="math inline">\(x\)</span> and <span class="math inline">\(x_{1:n}\)</span> with <span class="math inline">\(x_{1:n+1}\)</span> to include our new observation. This will have the posterior mean function <span class="math inline">\(\mu_{n+1}(\cdot)\)</span> defined as:</p>
<p><span class="math display">\[
\mu_{n+1}(x) = \mu_n(x) + \frac{\operatorname{cov}_n(x_{n+1},x)}{\operatorname{var}_n(x_{n+1}) + \sigma_{\mathrm{obs}}^2}(F(x_{n+1})|f(x_{1:n})-\mu_n(x_{n+1}))
\]</span></p>
<p>where <span class="math inline">\(\sigma_{obs}\)</span> is a noise variable which can be determined by the user <span class="citation" data-cites="ungredda2022efficientcomputationknowledgegradient">(<a href="#ref-ungredda2022efficientcomputationknowledgegradient" role="doc-biblioref">Ungredda, Pearce, and Branke 2022</a>)</span>. The conditional expected value for <span class="math inline">\(F(x_n^*)\)</span> changes to be:</p>
<p><span class="math display">\[
\mu_{n+1}^* := max_x\mu_{n+1}(x)
\]</span></p>
<p>So, we can see that the increase in the conditional expected value of <span class="math inline">\(F(x_n^*)\)</span> by sampling the new point <span class="math inline">\(x_{n+1}\)</span> is:</p>
<p><span class="math display">\[
\mu_{n+1}^* - \mu_n^*
\]</span></p>
<p>While this quantity is unknown before we sample <span class="math inline">\(x_{n+1}\)</span>, we can calculate it’s expected value given our observations <span class="math inline">\(x_1,...,x_n\)</span>. The Knowledge Gradient (KG) for sampling at a new point <span class="math inline">\(x\)</span> in the design space is defined as <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span class="math display">\[
KG_n(x) := E[\mu_{n+1}^* - \mu_n^*|x_{n+1} = x]
\]</span></p>
<p>The easiest way to calculate the KG is via simulation. This can be done by simulating one possible value for <span class="math inline">\(f(x_{n+1})\)</span> and then calculating <strong>?@eq-increaseinconditionalexpectedvalue</strong>. We iterate this process many times so that we can find the average of <span class="math inline">\(\mu_{n+1}^* - \mu_n^*\)</span> and this allows us to estimate <span class="math inline">\(KG_n(x)\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. This process, or calculating <strong>?@eq-KnowledgeGradient</strong> directly from the properties of the normal distribution, both work well in discrete, low dimensional problems which is the situation we are in for the first half of the project <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>We would sample the point <span class="math inline">\(x\)</span> with the largest <span class="math inline">\(KG_n(x)\)</span> as our next point <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
</section>
<section id="kmeans-process-for-selecting-multiple-points" class="level3">
<h3 class="anchored" data-anchor-id="kmeans-process-for-selecting-multiple-points">Kmeans process for selecting multiple points</h3>
<p>Combining a clustering method with an acquisition function was an idea I had early on in the project. I was then able to find literature on the subject, including using kmeans clustering.</p>
<p>Let us have a design space <span class="math inline">\(X\)</span> as before. We want to be able to select multiple points to sample in our next round so that we continue the pattern set up in the original paper, whilst keeping a good trade off between exploration and exploitation <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>,<span class="citation" data-cites="batchspreadingoutjustification">(<a href="#ref-batchspreadingoutjustification" role="doc-biblioref">Azimi, Fern, and Fern 2010</a>)</span>. So, we use the k-means clustering method, which is the most commonly used due to its simplicity compared to other clustering algorithms <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>.</p>
<p>This algorithm creates <span class="math inline">\(k\)</span> clusters (groups of points) such that the points within each cluster have the sum of squares to the centre of their cluster smaller than it would be to the centre of any other cluster <span class="citation" data-cites="kmeansdocumentation">(<a href="#ref-kmeansdocumentation" role="doc-biblioref">RDocumentation 2025</a>)</span>. It starts by defining <span class="math inline">\(k\)</span> centroids which should be placed as much as possible far away from each other <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. Then, we take each point in the space and associate it to the nearest centroid. We stop when every point has been assigned to a centroid <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. At this point, we re-calculate <span class="math inline">\(k\)</span> new centroids as the centers of the clusters created by the previous step. This may result in some points changing clusters <span class="citation" data-cites="AKMeansClusteringAlgorithm">(<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">Hartigan and Wong 1979</a>)</span>. We repeat this process until no points change clusters <span class="citation" data-cites="AKMeansClusteringAlgorithm">(<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">Hartigan and Wong 1979</a>)</span>,<span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>.</p>
<p>However, before the algorithm can start, we must specify how many clusters we want <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. This can be difficult in many cases <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. In our case, it is relatively simple as we know how many points we want to sample next and so we set this to be the number of clusters. Lastly, we run the algorithm on <span class="math inline">\(X\)</span> to form the clusters and then we pick the point with the highest value of the acquisition function from each cluster to sample in our next round.</p>
</section>
</section>
<section id="application-of-theory-in-my-project" class="level2">
<h2 class="anchored" data-anchor-id="application-of-theory-in-my-project">Application of theory in my project</h2>
<section id="set-up-the-gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="set-up-the-gaussian-processes">Set up the Gaussian Processes</h3>
<p>We are focusing on maximising the objective function from the paper which is given below <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>:</p>
<p><span class="math display">\[
f(\theta) = I_{[0.95,1]}(P(B(\theta)&gt;B_{lim})) \times C(\theta)
\]</span></p>
<p>where <span class="math inline">\(C(\theta)\)</span> is median long term catch, <span class="math inline">\(B(\theta)\)</span> is long-term <span class="math inline">\(SSB\)</span> and:</p>
<p><span class="math display">\[
I_{[0.95,1]}(x) =
\begin{cases}
1 &amp; \text{if   } x\in[0.95,1] \\
0 &amp; \text{otherwise }
\end{cases}
\]</span></p>
<p>is an indicator function.</p>
<p>To do this, Spence uses two GPs where the risk GP models <span class="math inline">\(\ln(risk)\)</span> and the catch GP models <span class="math inline">\(\ln(median\ catch)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Maintaining the notation from <span class="citation" data-cites="Originalpaper">Spence (<a href="#ref-Originalpaper" role="doc-biblioref">2025</a>)</span>, we use <span class="math inline">\(m_1\)</span> for the mean function of the catch GP and <span class="math inline">\(m_2\)</span> for the mean function of the risk GP <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>:</p>
<p><span class="math display">\[
\begin{split}
m_1 (\phi) = \beta_{1,0} + \beta_{1,1} (\operatorname{ln}(\phi_1 + 0.1))+\beta_{1,2}(\operatorname{ln}(\phi_1 + 0.1))^2 + \\
\beta_{1,3}(\operatorname{ln}(\phi_1 + 0.1))^3+ \beta_{1,4}(\phi_2\operatorname{ln}(\phi_1 + 0.1)) + \beta_{1,5}\phi_2
\end{split}
\]</span></p>
<p><span class="math display">\[
m_2(\phi) = \beta_{2,0} + \beta_{2,1}\phi_1 + \beta_{2,2}\phi_2 + \beta_{2,3}\phi_1\phi_2
\]</span></p>
<p>where all of the above <span class="math inline">\(\beta_{s,t}\)</span> for <span class="math inline">\(s \in \{1,2\}\)</span> and <span class="math inline">\(t \in \{1,2,3,4,5\}\)</span> are coefficients to be found through maximum likelihood estimation and:</p>
<p><span class="math display">\[
\phi_1 = \frac{F_{target}-0.1}{0.4} \quad \textrm{and} \quad \phi_2 = \frac{B_{trigger}-110000}{90000}
\]</span></p>
<p>as we have rescaled for numerical stability in the GP <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
<p>Our covariance function <span class="math inline">\(c\)</span> for both GPs is the variance <span class="math inline">\(\sigma_i^2\)</span> (which is acting as a scalar) times the Ornstein-Uhlenbeck correlation function <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>:</p>
<p><span class="math display">\[
r_i(\phi,\phi',\delta_i) = \operatorname{exp} \left(-\frac{|\phi_1 - \phi'_1|}{\delta_{i,1}} - \frac{|\phi_2 - \phi_2'|}{\delta_{i,2}}\right)
\]</span></p>
<p>where <span class="math inline">\({\delta_{i,1}}\)</span> and <span class="math inline">\({\delta_{i,2}}\)</span> are the length scales for each of the <span class="math inline">\(\phi_1\)</span> and <span class="math inline">\(\phi_2\)</span> terms respectively <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p>We need to sample our first round of eight points before setting up the GPs so that we have enough data to estimate all of the coefficients in our GPs <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>. Note that until we have sampled sixteen points, we set the prior of the catch GP to be the same as the risk GP, <span class="math inline">\(m_2(\phi)\)</span>, except with different coefficients <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This is because we need to estimate the coefficients for the mean function for the catch GP and the length scales for the covariance function <span class="math inline">\(c\)</span> from the same data <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>,<span class="citation" data-cites="Roustant2012DiceKrigingpaper">(<a href="#ref-Roustant2012DiceKrigingpaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012a</a>)</span>. For mathematical stability, these functions should have less than eight coefficients between them in our first round as we are only sampling eight points per round due to computational limitations encountered at the time of the Spence 2025 paper <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>,<span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. However, after our first round we reset the mean function for the catch GP to <span class="math inline">\(m_1(\phi)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
<p>We can set up Gaussian Processes (GPs) as described above in R using the DiceKriging package and use maximum likelihood estimation to get the hyperparameters of <span class="math inline">\(m_1(\phi),m_2(\phi)\)</span> and <span class="math inline">\(c\)</span> for our GPs <span class="citation" data-cites="DiceKrigingDocumentation">(<a href="#ref-DiceKrigingDocumentation" role="doc-biblioref">Roustant 2025</a>)</span>,<span class="citation" data-cites="Roustant2012DiceKrigingpaper">(<a href="#ref-Roustant2012DiceKrigingpaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012a</a>)</span>. Then, we use them to predict the <span class="math inline">\(\ln(median\ catch)\)</span> and <span class="math inline">\(\ln(risk)\)</span> at every point in the design space. We can then exponentiate these results where needed. We are building our GPs with <span class="math inline">\(\ln\)</span> of the values we want because this helps us generate better predictions <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
<p>We have been able to visualise the GPs after the first round in 3D:</p>
<p>GET THESE IN SOMEHOW - HAVE TO UPLOAD FINAL VER AS PDF SO MAYBE NOT? COULD ADD AS A SEPARATE APPENDIX TO THIS FILE ON THE GITHUB - MAY NEED OT ADD THE FILE FOR JUST THE GRAPH TO GITHUB FIRST</p>
<p>The middle plane is the mean of the risk GP, whereas the bottom and top planes represent the lower and upper bounds of the 95 percent confidence interval respectively. The planes meet at the evaluated points, which are the red dots on the diagram. The scales are odd due to the re-scaling of <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> and fitting our GPs to log of the values we want throughout the code which helps to keep the GP stable <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
<p>We can then also do this for the catch GP:</p>
<p>GET THESE IN SOMEHOW - HAVE TO UPLOAD FINAL VER AS PDF SO MAYBE NOT? COULD ADD AS A SEPARATE APPENDIX TO THIS FILE ON THE GITHUB - MAY NEED OT ADD THE FILE FOR JUST THE GRAPH TO GITHUB FIRST</p>
</section>
<section id="excluding-implausible-points" class="level3">
<h3 class="anchored" data-anchor-id="excluding-implausible-points">Excluding implausible points</h3>
<p>Recall that we have the GP for <span class="math inline">\(\ln(median\ catch)\)</span> which is modeling the <span class="math inline">\(C(\theta)\)</span> part of the objective function. As the first half of the objective function <span class="math inline">\(f\)</span> is an indicator function, we can focus on maximising <span class="math inline">\(C(\theta)\)</span> whilst meeting the threshold stated by the indicator function.</p>
<p>Firstly, we enforce the threshold in the indicator function by calculating <span class="math inline">\(P(\ln(risk)\le\ln(0.05))\)</span>, which is equivalent to the precautionary threshold <span class="math inline">\(P(risk\le0.05)\)</span>. We calculate this by making predictions for our risk at each point using our risk GP and finding the probability that these predictions for risk are below <span class="math inline">\(\ln(0.05)\)</span>. This probability can be found using <strong>?@eq-posteriordistributiongivensamples</strong>. Then, we exclude any points with <span class="math inline">\(P(\ln(risk)\le \ln(0.05))\le\ \varepsilon = 0.0001\)</span> by setting their value for the acquisition function equal to <span class="math inline">\(0\)</span>. This means they will not be chosen as a point to sample in the next round.</p>
<p>To maximise <span class="math inline">\(C(\theta)\)</span>, we use Bayesian history matching to speed up the process by removing any points that are implausible according to <strong>?@eq-removeimplausiblepoint</strong>.</p>
<p>This full process ensures that only points that meet the precautionary threshold and and are plausible according <strong>?@eq-removeimplausiblepoint</strong> can be selected.</p>
</section>
<section id="deciding-on-next-point-to-sample" class="level3">
<h3 class="anchored" data-anchor-id="deciding-on-next-point-to-sample">Deciding on next point to sample</h3>
<p>We have three different acquisition functions we have investigated using here.</p>
<section id="expected-improvement-1" class="level4">
<h4 class="anchored" data-anchor-id="expected-improvement-1">Expected Improvement</h4>
<p>We use the <strong>?@eq-EIincode</strong> expression and calculate the EI for every point in the sample space, setting those that don’t meet the precautionary threshold to zero as explained above.</p>
</section>
<section id="augmented-expected-improvement-1" class="level4">
<h4 class="anchored" data-anchor-id="augmented-expected-improvement-1">Augmented Expected Improvement</h4>
<p>As our situation has no noise, in our code we are still using <span class="math inline">\(f^*_n\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Thus, we use the equation:</p>
<p><span class="math display">\[
AEI_n(x_{n+1})=EI_n(x_{n+1})\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\]</span></p>
<p>to calculate the AEI for every point in the sample space. where we set <span class="math inline">\(\sigma_{obs} = 0\)</span>. We again make sure to set any points not meeting the precautionary threshold to have <span class="math inline">\(AEI = 0\)</span>.</p>
</section>
<section id="knowledge-gradient-1" class="level4">
<h4 class="anchored" data-anchor-id="knowledge-gradient-1">Knowledge Gradient</h4>
<p>We estimate the KG from 100 simulations and update the KG each round using <strong>?@eq-updatemuKG</strong> . As in the other acquisition functions, we make sure to set the value to zero for any point in the sample space not meeting the precautionary threshold.</p>
</section>
<section id="kmeans-process" class="level4">
<h4 class="anchored" data-anchor-id="kmeans-process">Kmeans process</h4>
<p>We have now been able to determine which points are possible based on the probability that their median catch is higher than the current maximum median catch (using Bayesian History Matching) and the probability that their risk is less than 0.05. These points will be called the Possible Space, <span class="math inline">\(PS\)</span>. We have then assigned a value to each point in <span class="math inline">\(PS\)</span> using one of the acquistion functions above. Now, we want to decide which 8 points are best to evaluate next.</p>
<p>We use the <code>kmeans</code> function which is part of the stats package in R to do this. This function uses the algorithm from <span class="citation" data-cites="AKMeansClusteringAlgorithm">Hartigan and Wong (<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">1979</a>)</span> by default and implements our kmeans algorithm described in the General Theory section <span class="citation" data-cites="kmeansdocumentation">(<a href="#ref-kmeansdocumentation" role="doc-biblioref">RDocumentation 2025</a>)</span>. We set <span class="math inline">\(k=8\)</span> to create eight clusters so that we select eight points to sample next round.</p>
<p>This method allows us to search for viable points by looking in the <span class="math inline">\(PS\)</span> but also to keep the points we are going to sample spread out so that we can balance exploitation and exploration more effectively <span class="citation" data-cites="batchspreadingoutjustification">(<a href="#ref-batchspreadingoutjustification" role="doc-biblioref">Azimi, Fern, and Fern 2010</a>)</span>.</p>
</section>
</section>
<section id="updating-our-gps" class="level3">
<h3 class="anchored" data-anchor-id="updating-our-gps">Updating our GPS</h3>
<p>In our second round, we need to update our GPs with new data <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. We do this by adding in the data for the points that have been newly evaluated this round. Hence, we can do the calculations from the Kriging section to update the GP with a new mean vector and covariance kernel for our next round. For the catch GP, we move on to using <span class="math inline">\(m_1(\phi)\)</span> from the second round onwards because by the time we create this GP we have sampled 16 points <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
<p>Now, we repeat the full process described in the Application of theory in my project section until there are no points with a <span class="math inline">\(KG &gt; 0\)</span> . Then, the optimal point is the <span class="math inline">\(x^*\)</span> that has the highest catch out of the precautionary points. In our case, <span class="math inline">\(x^*\)</span> is the <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that will give the highest median long-term catch whilst following the precautionary principle <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
</section>
</section>
<section id="experimenting-with-the-kernel-and-mean" class="level2">
<h2 class="anchored" data-anchor-id="experimenting-with-the-kernel-and-mean">Experimenting with the kernel and mean</h2>
<p>Along with my project supervisor, I decided to run some experiments determining whether changing the kernel and mean used in <code>case_study8.R</code> would have an effect on the value it determined as the optimal point. This was due to the fact that the exponential kernel, also called the Ornstein–Uhlenbeck correlation function, currently being used in the code (shown in <strong>?@eq-originalkernel</strong>) was not as smooth as the Gaussian kernel (general form shown in <strong>?@eq-GaussianKernel</strong>). This was pointed out and questioned by my supervisor as the Gaussian kernel is more widely used <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p><span class="math display">\[
r_i(\phi,\phi',\delta_i) = \textrm{exp}\left(-\frac{|\phi_1-\phi_1'|}{\delta_{i,1}} - \frac{|\phi_2-\phi_2'|}{\delta_{i,2}}\right)
\]</span></p>
<p><span class="math display">\[
r(\phi) = \textrm{exp}\left(-\frac{r^2}{2l^2}\right)
\]</span></p>
<p>The mean was also identified as a possible issue. By assuming a very specific mean (shown below <strong>?@eq-gpmean</strong>) for the Gaussian Process (GP) we are assuming that we know a lot about the objective function <span class="math inline">\(f\)</span> (which is what we are modeling with our GP) <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. Thus, it was suggested that I change the code to a zero mean which is commonly done when we do not want to make any assumptions about our objective function <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p><span class="math display">\[
\begin{split}
m_1(\phi) = \beta_{1,0} + \beta_{1,1}(ln(\phi_1 +0.1))+\beta_{1,2}(ln(\phi_1+0.1))^2+ \\
\beta_{1,3}(ln(\phi_1+0.1))^3+\beta_{1,4}(\phi_2ln(\phi_1+0.1))+\beta_{1,5}\phi_2
\end{split}
\]</span></p>
<p>where <span class="math display">\[\phi_1 = \frac{F_{target}-0.1}{0.4} \quad \textrm{and} \quad \phi_2 = \frac{B_{trigger}-110000}{90000}\]</span></p>
<p>and all of the above <span class="math inline">\(\beta_{s,t}\)</span> for <span class="math inline">\(s \in \{1,2\}\)</span> and <span class="math inline">\(t \in \{1,2,3,4,5\}\)</span> are coefficients to be found.</p>
<section id="experimenting-with-the-kernel" class="level3">
<h3 class="anchored" data-anchor-id="experimenting-with-the-kernel">Experimenting with the kernel</h3>
<p>The assumption made by the Gaussian kernel of infinite differentiability can be too strong for real-world processes as it is very smooth <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. The GP will create a very smooth surface due to the very smooth assumption in the Gaussian kernel <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. If a new point we sample does not fit this assumption, then the GP is unsure how to proceed and so sets the variance very high as if it knew nothing, sometimes called a variance explosion <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. This results in many points that had been deemed implausible returning and this is likely to continue unless different sampling points are chosen <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. It is instead recommended that we use a kernel from the Matern class of kernels, which includes the exponential kernel <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p>My experiments appear to back up this theory. Using the exponential kernel, in our case the GP converges to a solution after seven rounds. After changing the kernel to be the Gaussian kernel or the Matern kernel with <span class="math inline">\(\nu = \frac{5}{2}\)</span> it can be seen that neither of these alternatives converge after seven rounds . In each heatmap below, we are plotting how likely the model thinks it is that this point will be a solution with higher catch, but still precautionary. Any areas with a probability less than <code>1e-04</code> of having a higher catch than the current best and being precautionary have been ruled out as implausible and are displayed as dark blue.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/case_study8_exp_kernel_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Exponential kernel</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/case_study8_gauss_kernel_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Gaussian kernel</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/case_study_matern5_2_kernel_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Matern 5/2 kernel</figcaption>
</figure>
</div>
<p>Similar results occur when we run the method once it has been augmented by an acquisition function. As we then allow the method to continue until it has converged, we find that the different kernels cause convergence in a different number of rounds. For example, <code>looped_ver_case_study8_mult_point_EI.R</code> converges after seven rounds with the exponential kernel, but after changing to the Gauss kernel it converges after twenty five rounds. When using the Matern 5/2 kernel, the GP does not converge to the correct point.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ei_looped_exp_kernel_round_7.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: case_study8.R with EI at seven rounds</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/looped_ei_gauss_kernel_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: case_study8.R with EI using Gauss kernel at seven rounds</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/looped_EI_matern_5_2_non_converge.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6: Last round of case_study8.R with EI function using Matern 5/2 kernel</figcaption>
</figure>
</div>
</section>
<section id="experimenting-with-the-mean" class="level3">
<h3 class="anchored" data-anchor-id="experimenting-with-the-mean">Experimenting with the mean</h3>
<p>Setting the mean of the GP to zero as in <a href="#lst-mean_zero" class="quarto-xref">Listing&nbsp;1</a> is more often done in examples to showcase the key concepts than when implementing these methods in real-world situations <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. My experiments seem to back up this theory as they again show results that are the same or worse than our original approach.</p>
<div id="lst-mean_zero" class="R listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mean_zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: Setting the GP Mean to Zero
</figcaption>
<div aria-describedby="lst-mean_zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode R code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>gp_cat <span class="ot">&lt;-</span> <span class="fu">km</span>(<span class="sc">~</span><span class="dv">0</span>,<span class="at">design=</span>runs[,<span class="fu">c</span>(<span class="st">"Ftarget"</span>,<span class="st">"Btrigger"</span>)],<span class="at">estim.method=</span><span class="st">"MLE"</span>,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="at">response =</span> res_cat,<span class="at">nugget=</span><span class="fl">1e-12</span><span class="sc">*</span><span class="fu">var</span>(res_cat),<span class="at">covtype =</span> <span class="st">"exp"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<p>We can see from the heatmaps below that the simulations with means changed to zero either converge at the same round or later on than when we use the original mean specified in <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This is likely to be because we are able to encode information that we already know about how fisheries and stocks operate when using the original mean, which helps the GP to narrow down to the optimal point in less rounds <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>, <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mean_zero_first_roudn_only.png" class="img-fluid figure-img"></p>
<figcaption>Figure 7: case_study8.R with the mean of the GP set to zero in the first round only at the seventh round</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mean_zero_all_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 8: case_study8.R with the mean of the GP set to zero in all rounds at the seventh round</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/looped_kg_mean_zero_first_round_after_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 9: case_study8.R augmented with the KG acquisition function with the mean of the GP set to zero in the first round at the seventh round</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/looped_kg_mean_zero_every_round_7_rounds.png" class="img-fluid figure-img"></p>
<figcaption>Figure 10: case_study8.R augmented with the KG acquisition function with the mean of the GP set to zero in all rounds at the seventh round</figcaption>
</figure>
</div>
</section>
</section>
<section id="determining-the-best-acquisition-function" class="level2">
<h2 class="anchored" data-anchor-id="determining-the-best-acquisition-function">Determining the best acquisition function</h2>
<section id="timing-test" class="level3">
<h3 class="anchored" data-anchor-id="timing-test">Timing test</h3>
<p>The results from this test vary, but only by a few milliseconds. This is likely due to using a different runner (where the code is running) each time on Github as there can be background processes, etc. It could also be due to slightly different loading times for the R packages each time. Any random sampling has been fixed by setting a seed.</p>
<p>On average, after five runs:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>case_study8<span class="sc">:</span> <span class="fl">2.1716514636</span> </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_KG<span class="sc">:</span> <span class="fl">3.2781787488</span> </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_AEI<span class="sc">:</span> <span class="fl">2.21322831978</span> </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_EI<span class="sc">:</span> <span class="fl">2.13455526</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>We can see that <code>looped_ver_case_study8_multi_point_EI</code> is the quickest to run but is close to <code>case_study8.R</code>. <code>looped_ver_case_study8_multi_point_AEI</code> is also very close. However, <code>looped_ver_case_study8_KG</code> takes much longer to run. We need to bear in mind that these are all with the pre-evaluated objective function. This means we do not have to evaluate the objective function, we only have to look up the pre-evaluated value, which vastly reduces the running time.</p>
</section>
<section id="total-evaluations-test" class="level3">
<h3 class="anchored" data-anchor-id="total-evaluations-test">Total evaluations test</h3>
<p>These results do not vary, and we get the output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="sc">==</span><span class="er">=</span> Total Evaluations Summary <span class="sc">==</span><span class="er">=</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>case_study8 <span class="ot">=</span> <span class="dv">54</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_KG <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_AEI <span class="ot">=</span> <span class="dv">56</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_EI <span class="ot">=</span> <span class="dv">56</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This clearly shows that the KG acquisition function requires the least amount of evaluations of the objective function. When we do not have pre-evaluated points, this will make it the least expensive to run.</p>
</section>
<section id="number-of-rounds-test" class="level3">
<h3 class="anchored" data-anchor-id="number-of-rounds-test">Number of Rounds test</h3>
<p>The results from this test don’t change.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="sc">==</span><span class="er">=</span> Rounds Completed Summary <span class="sc">==</span><span class="er">=</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>case_study8 <span class="ot">=</span> <span class="dv">7</span> rounds</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_KG <span class="ot">=</span> <span class="dv">7</span> rounds</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_AEI <span class="ot">=</span> <span class="dv">7</span> rounds</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>looped_ver_case_study8_multi_point_EI <span class="ot">=</span> <span class="dv">7</span> rounds</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>As we evaluate the objective function in parallel for however many points we pick each round (normally eight, but it may not be in the last round), this means they will all be even in terms of timing.</p>
</section>
<section id="convergence-test" class="level3">
<h3 class="anchored" data-anchor-id="convergence-test">Convergence Test</h3>
<p>The results show that all the files converge to the correct answer in 100/100 runs. There may be slight GP fit variation which could lead to slightly different EI ordering and so slightly different selected next points, but after 100 runs we can be relatively confident that this does not lead to the process converging to the wrong point.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Despite currently taking more time on average than the other methods, the KG acquisition function method will be the best to use here because it requires less evaluations and so is less expensive to run. However, it does have the same number of rounds as the others, which is important because we evaluate the new points each round in parallel, and so it doesn’t have an overwhelming lead here. The results from the convergence test are essentially irrelevant for this comparison as all methods got the same result.</p>
</section>
</section>
</section>
<section id="applying-the-framework-to-a-mixed-fisheries-context" class="level1">
<h1>Applying the framework to a mixed fisheries context</h1>
<p>LIKELY TO BE COVERED IN THE CONTENTS TABLE</p>
<p>What mathematical methods will I explain in this document that have not been explained in Explanation of Mathematical Methods from First Half of the Project?</p>
<ul>
<li><p>The basics of the MixME model</p></li>
<li><p>Creating the operating model</p></li>
<li><p>Building the MixME input object</p></li>
<li><p>Running the simulation</p></li>
<li><p>Analysing the results of the simulation</p></li>
<li><p>HCR parameters</p></li>
<li><p>Additions or differences in the shortcut model</p></li>
<li><p>Calculating the risk</p></li>
<li><p>Parallelisation</p></li>
</ul>
<section id="context-1" class="level2">
<h2 class="anchored" data-anchor-id="context-1">Context</h2>
<p>The aim of this half of the project is to apply the methods from the first half of the project to mixed fisheries using the MixME R package. I have used the methods from the first half of the project to write code that follows a very similar process, but where the aim is now to find the <span class="math inline">\(F_{target}\)</span> for each stock that is precautionary but maximises total catch over the years 2030 to 2039. I set this goal because MixME is designed to run projections into the future, and looking at catch in the long term will minimise the chance that a simulation where a stock fails is chosen <span class="citation" data-cites="MixME">(<a href="#ref-MixME" role="doc-biblioref">Pace et al. 2025a</a>)</span>,<span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. The 20 year projection from 2020-2039 was kept consistent with examples on the MixME documentation and follows guidelines from ICES to create long-term projections based on the biology of the stocks <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>,<span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>. It also follows ICES guidelines to only calculate catch and risk for the last ten years for long-term projection, to allow time for a recovery period <span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>.</p>
<p>I focused on the datasets from the Fixed fishing mortality management strategy example (mixedfishery_MixME_om) and the Exploring simulation outputs example (mixedfishery_MixME_input) which are both in the MixME documentation <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. However, for the second dataset mixedfishery_MixME_input I was given a shortcut method by a researcher at Cefas which takes a more direct approach to the simulation. The code for these datasets is Optimising_ftarget_in_MixME_mult_points_parallel.R and Two_stocks_Optimising_ftarget_in_shortcut_model.R respectively.</p>
<p>Both of these datasets have two stocks (cod and haddock) and two fleets <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. I was told by the same researcher at Cefas that the stocks are North Sea cod and Celtic Sea haddock, but using citations I can only back up that they are Atlantic cod and haddock <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>,<span class="citation" data-cites="ICESCodFactsheet">(<a href="#ref-ICESCodFactsheet" role="doc-biblioref">ICES, n.d.</a>)</span>,<span class="citation" data-cites="ICESCodFactsheet">(<a href="#ref-ICESCodFactsheet" role="doc-biblioref">ICES, n.d.</a>)</span>. This allows me to use the same methods as in the first half of the project by replacing <span class="math inline">\(F_{target}\)</span> with <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(B_{trigger}\)</span> with <span class="math inline">\(F_{had}\)</span>, where <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span> are the fishing mortalities for cod and haddock respectively.</p>
<p>For this half of the project, we have switched to modelling SSB directly instead of calculating the risk. This is due to the simulation being deterministic as both datasets only have one iteration and the noise for this is pre-calculated <span class="citation" data-cites="MixMESupp">(<a href="#ref-MixMESupp" role="doc-biblioref">Pace et al. 2025b</a>)</span>. These conditions simplify the simulation but unfortunately mean that we cannot use the standard ICES definition of risk to calculate <span class="math inline">\(Risk = P(SSB &lt; B_{lim})\)</span> <span class="citation" data-cites="MixMESupp">(<a href="#ref-MixMESupp" role="doc-biblioref">Pace et al. 2025b</a>)</span>,<span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>. Instead, we extract <span class="math inline">\(\textrm{min}(SSB)\)</span> for each sampled point from the years 2030-2039 in the simulation and then model these as a GP <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>,<span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>. This allows us to predict the distribution of <span class="math inline">\(\textrm{min}(SSB)\)</span> values at each point in the design space <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. For each point, we then see how many of these predicted values fall below <span class="math inline">\(B_{lim}\)</span> to calculate <span class="math inline">\(P(\textrm{min}(SSB) &lt; B_{lim})\)</span> at that point <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. We do this for each stock. This method of calculation satisfies the ICES precautionary standard <span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>.</p>
<p>Due to the change in stocks being modelled, I have decided to keep the GP prior for catch modelled by the ~.^2 function in R for every round of the optimisation process. Despite the more complicated prior used in later rounds in case_study8.R being designed to approximate yield curves, it may not be appropriate in a mixed fisheries context where the catch of one species is affected by the catch of another <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>,<span class="citation" data-cites="MixME">(<a href="#ref-MixME" role="doc-biblioref">Pace et al. 2025a</a>)</span>,<span class="citation" data-cites="ULRICH201238">(<a href="#ref-ULRICH201238" role="doc-biblioref">Ulrich et al. 2012</a>)</span>. Leaving the GP more general avoids mis-specification, ensuring that the GP can be appropriately fitted to the points <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>.</p>
<p>The process for the first half of the project adapted to our new situation is outlined below. We take a Bayesian History Matching (BHM) approach <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Firstly, to get some initial data, we randomly sample our first set of <span class="math inline">\(n-1\)</span> points, where <span class="math inline">\(n\)</span> is the number of cores the system we are on has <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Then, for each round we do the following:</p>
<p>We set up or update the Gaussian Processes (GPs) to model the <span class="math inline">\(\textrm{min}(SSB)\)</span> from 2030-2039 for each stock and the GP to model the total catch from 2030-2039</p>
<p>We can then use the <span class="math inline">\(B_{lim}\)</span> for each stock as a threshold so that we only consider the values of <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span> that have <span class="math inline">\(P(min(SSB)&gt;B_{lim})&gt;0.05\)</span> for all of the last ten years of the simulation (2030-2039)</p>
<p>We use the GP which is modelling the total catch to predict the value for the total catch at every point in the sample space, which will have some uncertainty</p>
<p>We use BHM to remove any points that are implausible (that have a probability less than 0.01% of being higher than the current best total catch)</p>
<p>We use the Knowledge Gradient (KG) acquisition function to select <span class="math inline">\(n-1\)</span> plausible points to sample in the next round, as per the discussion in Deciding which acquisition function is best</p>
<p>We repeat this process until there is only one plausible point left and then we will accept this as being the <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span> that maximise the catch whilst keeping the <span class="math inline">\(SSB\)</span> above <span class="math inline">\(B_{lim}\)</span>.</p>
</section>
<section id="calculating-the-risk" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-risk">Calculating the risk</h2>
<p>MixME defines risk as the proportion of iterations in each year where <span class="math inline">\(SSB\)</span> falls below <span class="math inline">\(B_{lim}\)</span> <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. However, due to only having one iteration in each of my datasets, I have experimented with using a different method to measure the risk <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
<p>In contrast to the first half of the project, we now calculate the risk using the <span class="math inline">\(SSB\)</span>. We have described it above briefly but will go into more detail here. We should quickly note before our calculations that our new sample space is a grid with all the combinations of <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span> both ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(0.6\)</span> in <span class="math inline">\(0.02\)</span> increments <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This range ensures that we are able to model stock collapse for cod by modelling values above <span class="math inline">\(F_{lim}\)</span> and that we can model stock recovery for cod by including very low values <span class="citation" data-cites="ICES2019CodAdvice">(<a href="#ref-ICES2019CodAdvice" role="doc-biblioref">ICES 2019c</a>)</span>. It also allows us to model unsafe fishing for haddock by modelling values above <span class="math inline">\(F_{MSY}\)</span> <span class="citation" data-cites="ICES2019HaddockAdvice">(<a href="#ref-ICES2019HaddockAdvice" role="doc-biblioref">ICES 2019b</a>)</span>. Recalling that cod is the choke stock for both of our datasets, this sample space is appropriate <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>,<span class="citation" data-cites="MixedFisheriesStatusandManagement2023">(<a href="#ref-MixedFisheriesStatusandManagement2023" role="doc-biblioref">Sun et al. 2023</a>)</span>,<span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>.</p>
<p>We also have a <span class="math inline">\(B_{lim}\)</span> for each stock, taken from ICES advice <span class="citation" data-cites="ICES2020HaddockBlim">(<a href="#ref-ICES2020HaddockBlim" role="doc-biblioref">ICES 2020b</a>)</span>,<span class="citation" data-cites="ICES2020CodBlim">(<a href="#ref-ICES2020CodBlim" role="doc-biblioref">ICES 2020a</a>)</span>. The <span class="math inline">\(B_{lim}\)</span> for cod is <span class="math inline">\(107,000\)</span> tonnes and the <span class="math inline">\(B_{lim}\)</span> for haddock is <span class="math inline">\(9,227\)</span> tonnes.</p>
<p>Firstly, we extract the <span class="math inline">\(\textrm{min}(SSB)\)</span> for each stock from the result of the simulation we have run using the tracking object <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>,<span class="citation" data-cites="MixME">(<a href="#ref-MixME" role="doc-biblioref">Pace et al. 2025a</a>)</span>. We then put these results into the GPs we will use to model <span class="math inline">\(\textrm{min}(SSB)\)</span> for each stock. These GPs have remained modelled in the same way as risk from the first half of the project to avoid mis-specification <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>,<span class="citation" data-cites="DiceKrigingPaper">(<a href="#ref-DiceKrigingPaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012b</a>)</span>.</p>
<p>They have also kept the same estimation method, nugget and kernel <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. This is because Maximum Likelihood Estimation is one of the standard methods used in the literature for our situation [<span class="citation" data-cites="williams2006gaussian">Williams and Rasmussen (<a href="#ref-williams2006gaussian" role="doc-biblioref">2006</a>)</span>]<span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. Furthermore, the <span class="math inline">\(SSB\)</span> for each stock is not necessarily smooth. Due to our discrete time steps, overfishing or low recruitment can cause sudden drops. This means we need a kernel which can appropriately model these sudden drops seen in this real-world process. As justified in Kernel and Mean Experiments the exponential kernel is appropriate for this. Our simulation is still deterministic and we still need our matrices to be invertible and so we keep the small nugget term <span class="citation" data-cites="DiceKrigingDocumentation">(<a href="#ref-DiceKrigingDocumentation" role="doc-biblioref">Roustant 2025</a>)</span>.</p>
<p>Next, we predict the <span class="math inline">\(\textrm{min}(SSB)\)</span> for every point in the sample space and calculate <span class="math inline">\(Risk = P(min(SSB) &lt; B_{lim})\)</span> for each stock. This can be done using a normal distribution because our Gaussian Process is a multivariate normal distribution <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref">Williams and Rasmussen 2006</a>)</span>. Explicitly, it can be found using the equation below which we already touched on in the first half of the project <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span class="math display">\[
F(x_{n+1})|f(x_{1:n}) \sim N(\mu_n(x_{n+1}), \sigma^2_n(x_{n+1}))
\]</span></p>
<p>Then, we set the KG of any unsafe points to be zero so that they will not be chosen as points to be sampled in the future.</p>
<p>By ensuring that <span class="math inline">\(P(\textrm{min}(SSB) &lt; B_{lim}) &lt; 0.05\)</span> we ensure that <span class="math inline">\(P(SSB &lt; B_{lim})&lt;0.05\)</span> for every year in the long term forecast (2030-2039). This guarantees that the maximum annual risk in this period remains below the 5 percent threshold, which is exactly what is required to meet the ICES precautionary standard due to their definition of <span class="math inline">\(Prob3\)</span> in <span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>. - CHECK AND ADD IN ABOUT BEING DETERMINISTIC AT LEAST. OR SAY MEETS PROB2 WHICH IS MORE RESTRICTIVE THAN PROB3</p>
<p>This means that this risk calculation could be used in policy documents to set official catch limits in countries that have agreed to this standard <span class="citation" data-cites="ICES2019WKGMSE2">(<a href="#ref-ICES2019WKGMSE2" role="doc-biblioref">ICES 2019a</a>)</span>.</p>
</section>
<section id="the-basics-of-the-mixme-model" class="level2">
<h2 class="anchored" data-anchor-id="the-basics-of-the-mixme-model">The basics of the MixME model</h2>
<section id="starting-f_cod-and-f_had" class="level3">
<h3 class="anchored" data-anchor-id="starting-f_cod-and-f_had">Starting <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span></h3>
<p>We started with <span class="math inline">\(F_{cod} = 0.28\)</span> and <span class="math inline">\(F_{had} = 0.353\)</span> based upon the values given in the Fixed fishing mortality management strategy example from the MixME wiki <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. These were drawn from the North Sea Cod and Celtic Sea Haddock advice published in 2020 respectively <span class="citation" data-cites="ICES2021Cod028">(<a href="#ref-ICES2021Cod028" role="doc-biblioref">ICES 2021</a>)</span>,<span class="citation" data-cites="ICES2020HaddockBlim">(<a href="#ref-ICES2020HaddockBlim" role="doc-biblioref">ICES 2020b</a>)</span>.</p>
</section>
<section id="creating-the-operating-model" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-operating-model">Creating the Operating model</h3>
<p>We first create the operating model mixedfishery_MixME_om which contains the true data for the stocks and fleets in the dataset <span class="citation" data-cites="MixME">(<a href="#ref-MixME" role="doc-biblioref">Pace et al. 2025a</a>)</span>. We have data for North Sea Cod from 1963-2019 and for Celtic Sea Haddock from 1993-2019 <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. For the stocks, this includes the numbers, natural mortality, stock mean individual weight and proportion mature split by age and for fleets this includes landing numbers, landing mean individual weight, discard numbers, discard mean individual weight, selectivity and catchability <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
<p>In both of our files, this is loaded with the dataset <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. This true data is generated using standard age-structured equations to model the dynamics of the fleets and the stocks <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. For every year, it has the catch (in terms of landings and discards) from each fleet and the survivors from that year <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. It also contains a stock-recruitment model and recruitment is done at the beginning of each time step <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
<p>The steps for fully assembling the operating model for input into the MixME model are:</p>
<ul>
<li><p>Estimate historic quota-share for the two fleets</p></li>
<li><p>Project structures 20 years into the future</p></li>
<li><p>Calculate numbers of both stocks in initial year</p></li>
<li><p>Generate an observation error model</p></li>
</ul>
<section id="estimate-historic-quota-share-for-the-two-fleets" class="level4">
<h4 class="anchored" data-anchor-id="estimate-historic-quota-share-for-the-two-fleets">Estimate historic quota-share for the two fleets</h4>
<p>Firstly, we use mixedfishery_MixME_om to determine the quota share of the catch for each fleet for each year in our projection. This is done by assuming for each stock that the quota share corresponds to the proportional share of landings and is carried out by the calculateQuotashare function <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
</section>
<section id="project-structures-20-years-into-the-future" class="level4">
<h4 class="anchored" data-anchor-id="project-structures-20-years-into-the-future">Project structures 20 years into the future</h4>
<p>To carry out our 20 year projection, we need to extend the stock and fishery structures forward from 2019 into 2039. There are three categories of parameters that are not estimated dynamically and so need to be extended <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. These are:</p>
<ul>
<li><p>All stock parameters, landings and discards mean individual weights and landed fraction</p></li>
<li><p>Catchability and catch selection</p></li>
<li><p>Quota-share</p></li>
</ul>
<p>We project the parameters in each of these categories from 2019-2039 using the average from the last three years <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. This means we fill in the actual values we will use, instead of calculating a percentage that is then used in a further calculation like in the quota share section <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
</section>
<section id="calculate-numbers-of-both-stocks-in-initial-year" class="level4">
<h4 class="anchored" data-anchor-id="calculate-numbers-of-both-stocks-in-initial-year">Calculate numbers of both stocks in initial year</h4>
<p>To be able to do our projections of catch, we need to know the number of each stock in each age class at the beginning of the first projection year, 2020. This requires us to do a 1-year short term forecast from our starting point of 2019 using the FLasher package <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We set an arbitrary forecast target for this forecast <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. This is because the forecast target is designed to tell the simulation what to aim for by the end of 2020, but we want the stock numbers at the beginning of 2020 and so this has no effect on the result we want <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
</section>
<section id="creating-the-observation-error-model" class="level4">
<h4 class="anchored" data-anchor-id="creating-the-observation-error-model">Creating the Observation Error Model</h4>
<p>This is another important component of the MixME model <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We create the observation error model stk_oem where we apply pre-sampled noise to the catch from each fleet (which we obtain from the mixedfishery_MixME_om object) <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We generate future stock and management advice from this object <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. - CONCISE AS IS SO NOT SURE IF NEED MORE DETAIL</p>
</section>
</section>
<section id="build-the-mixme-input-object" class="level3">
<h3 class="anchored" data-anchor-id="build-the-mixme-input-object">Build the MixME input object</h3>
<p>We then use the operating model in the makeMixME function to make the MixME input object, which can then be used to run the simulation. We have five arguments for this function. Two of these are where we input our Operating Model and our Observation Error model. The next two specify how many time steps (in our scenario, years) it takes for the management advice to be enacted and what type of management we are using respectively <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. The last argument decides whether or not parallel processing should be used <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We have set it to FALSE, mostly as it’s usage is not currently well described.</p>
</section>
<section id="running-the-simulation" class="level3">
<h3 class="anchored" data-anchor-id="running-the-simulation">Running the simulation</h3>
<p>Before running the simulation, we update some of the management and simulation settings <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We update the arguments for when catches occur, the age range for calculating average fishing mortality in all the places it is needed and the target fishing mortality we are using for this round of the simulation <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. - NOT ENOUGH INFORMATION WHY PROVIDED ON WIKI seems they may have purposefully set things up wrong to show customisation possibilities</p>
<p>We then run the simulation <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. It is important to note that the <span class="math inline">\(F_{cod}\)</span> and <span class="math inline">\(F_{had}\)</span> we choose at the start remain constant throughout the simulation <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
</section>
<section id="analysing-results-of-the-simulation" class="level3">
<h3 class="anchored" data-anchor-id="analysing-results-of-the-simulation">Analysing results of the simulation</h3>
<p>The tracking object records summary statistics for the modelled stock and fleet dynamics, as well as metrics describing the observed state of the system by the management procedure. As well as this, it contains the catch and the <span class="math inline">\(SSB\)</span> from each year of the simulation, which is how we have been extracting these throughout this document <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
<p>It also contains simulation performance and diagnostics statistics, which is what we will focus on here <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. We check for management advice failure, effort optimisation failure and the message given if there was any failure <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. Effort optimisation failure can be sued to see if we are over-fishing the stocks to a point that they will go extinct <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. Other results we can see are the over-quota catches and the quota uptake <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. Lastly, we can check which stock is the choke stock <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
</section>
<section id="hcr-parameters" class="level3">
<h3 class="anchored" data-anchor-id="hcr-parameters">HCR parameters</h3>
<p>We add <span class="math inline">\(B_{lim}\)</span> into the HCR to help us calculate the risk later on. It is helpful for calculating risk in our current way, or the way MixME defines it. - NO INFORMATION ON WIKI</p>
</section>
<section id="additions-or-differences-in-the-shortcut-model" class="level3">
<h3 class="anchored" data-anchor-id="additions-or-differences-in-the-shortcut-model">Additions or Differences in the Shortcut Model</h3>
<p>This is the Two_stocks_Optimising_ftarget_in_shortcut_model.R file. We use the same data here as in the Optimising_ftarget_in_MixME_mult_points_parallel.R file, except that we have now also loaded in the Operating Model and Observation Error Model with the data <span class="citation" data-cites="MixMewiki">(<a href="#ref-MixMewiki" role="doc-biblioref"><strong>MixMewiki?</strong></a>)</span>.</p>
<p>In this simulation, we have a condition meaning we return zero-catch advice if the <span class="math inline">\(SSB\)</span> is below <span class="math inline">\(B_{lim}\)</span> in the year after the advice year. We are checking the year after the advice year due to our management lag of one year. Firstly, we identify any simulation where <span class="math inline">\(SSB &lt;B_{lim}\)</span> and re-run these simulations specifically targeting <span class="math inline">\(B_{lim}\)</span>. However, if this still fails, we identify these runs in the zeroTAC variable and manually set the total annual catch (TAC) to zero. There is not very much information this procedure as the tutorial has not yet been added to the MixME wiki <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>.</p>
<p>We have a new way to create the HCR, the ICES_HCR function. We also have a new way to projects forwards into the next year to set an appropriate TAC, the forecast_fun function. We call the forecast_fun for every year of the simulation and it is similar to the short term forecast we used in the last file. There is not very much information on either of these functions as the tutorials have not yet been added to the MixME wiki <span class="citation" data-cites="MixMEwiki">(<a href="#ref-MixMEwiki" role="doc-biblioref">Pace 2024</a>)</span>. Lastly, we set up the forecast arguments in a similar way to before, but make sure to set them separately for each stock.</p>
</section>
</section>
<section id="parallelisation" class="level2">
<h2 class="anchored" data-anchor-id="parallelisation">Parallelisation</h2>
<p>This process has been set up so that it is easy to run in parallel. We are able to load in the data we need once and then run the simulations for each point we have chosen to sample in the round in parallel. As they run, each process only gathers the values we need and removes the rest of the results of the simulation to save memory. Here, the values we need are the <span class="math inline">\(min(SSB)\)</span> for cod, the <span class="math inline">\(min(SSB)\)</span> for haddock and the total catch of cod and haddock all over the years 2030-2039.</p>
<p>Then, we collect these results and set up the GPs for each of them. After this, we run through the process of removing any implausible points and selecting the points to sample in the next round in a very similar way to the first half of this project <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. We iterate this process until the optimal point(s) are found. - MULTIPLE JUSTIFIED WHEN ANALYSING RESULTS</p>
<p>The process above makes up one run of the script. To ensure that we are consistently receiving the same point(s) as our optimal point(s), we want to follow on from the original paper and run this script 1000 times <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This requires a new kind of parallelisation. We were able to accomplish this by creating an array of jobs which will run when there is free space on the Viking HPC used by the Univeristy of York <span class="citation" data-cites="VikingDocumentation">(<a href="#ref-VikingDocumentation" role="doc-biblioref">York, n.d.</a>)</span>. This allowed us to submit one job that would run the script 1000 times, making it a lot easier to do this than having to submit 1000 separate jobs <span class="citation" data-cites="VikingDocumentation">(<a href="#ref-VikingDocumentation" role="doc-biblioref">York, n.d.</a>)</span>. We also allowed up to 50 scripts to run at once on separate nodes, speeding up the process significantly <span class="citation" data-cites="VikingDocumentation">(<a href="#ref-VikingDocumentation" role="doc-biblioref">York, n.d.</a>)</span>.</p>
</section>
</section>
<section id="further-areas-for-development" class="level1">
<h1>Further areas for Development</h1>
<section id="moving-to-a-more-complicated-dataset" class="level2">
<h2 class="anchored" data-anchor-id="moving-to-a-more-complicated-dataset">Moving to a more complicated dataset</h2>
<p>Mention the one in the MixME paper.</p>
</section>
<section id="doing-a-proper-risk-calculation" class="level2">
<h2 class="anchored" data-anchor-id="doing-a-proper-risk-calculation">Doing a proper risk calculation</h2>
<p>I now meet precautionary, so would it be much of an improvement to use probabilities instead?</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-batchspreadingoutjustification" class="csl-entry" role="listitem">
Azimi, Javad, Alan Fern, and Xiaoli Fern. 2010. <span>“Batch Bayesian Optimization via Simulation Matching.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta. Vol. 23. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf</a>.
</div>
<div id="ref-BayesianOptimisationTutorial" class="csl-entry" role="listitem">
Frazier, Peter. 2018. <span>“A Tutorial on Bayesian Optimization.”</span> <a href="https://doi.org/10.48550/arXiv.1807.02811">https://doi.org/10.48550/arXiv.1807.02811</a>.
</div>
<div id="ref-AKMeansClusteringAlgorithm" class="csl-entry" role="listitem">
Hartigan, J. A., and M. A. Wong. 1979. <span>“A k-Means Clustering Algorithm.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 28 (1): 100–108. <a href="https://doi.org/10.2307/2346830">https://doi.org/10.2307/2346830</a>.
</div>
<div id="ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" class="csl-entry" role="listitem">
Huang, D., Theodore Allen, William Notz, and Ning Zheng. 2006. <span>“Global Optimization of Stochastic BlackBox Systems via Sequential Kriging Meta-Models.”</span> <em>Journal of Global Optimization</em> 34: 441–66. <a href="https://doi.org/10.1007/s10898-005-2454-3">https://doi.org/10.1007/s10898-005-2454-3</a>.
</div>
<div id="ref-ICES2019WKGMSE2" class="csl-entry" role="listitem">
ICES. 2019a. <span>“<span class="nocase">Workshop on Guidelines for Management Strategy Evaluations (WKGMSE2)</span>,”</span> January. <a href="https://doi.org/10.17895/ices.pub.5531">https://doi.org/10.17895/ices.pub.5531</a>.
</div>
<div id="ref-ICES2019HaddockAdvice" class="csl-entry" role="listitem">
———. 2019b. <span>“<span class="nocase">Haddock (Melanogrammus aeglefinus) in divisions 7.b–k (southern Celtic Seas and English Channel)</span>,”</span> June. <a href="https://doi.org/10.17895/ices.advice.4785">https://doi.org/10.17895/ices.advice.4785</a>.
</div>
<div id="ref-ICES2019CodAdvice" class="csl-entry" role="listitem">
———. 2019c. <span>“<span class="nocase">Cod (Gadus morhua) in Subarea 4, Division 7.d, and Subdivision 20 (North Sea, eastern English Channel, Skagerrak)</span>,”</span> November. <a href="https://doi.org/10.17895/ices.advice.5640">https://doi.org/10.17895/ices.advice.5640</a>.
</div>
<div id="ref-ICES2020CodBlim" class="csl-entry" role="listitem">
———. 2020a. <span>“<span class="nocase">Cod (Gadus morhua) in Subarea 4, Division 7.d, and Subdivision 20 (North Sea, eastern English Channel, Skagerrak)</span>,”</span> June. <a href="https://doi.org/10.17895/ices.advice.5891">https://doi.org/10.17895/ices.advice.5891</a>.
</div>
<div id="ref-ICES2020HaddockBlim" class="csl-entry" role="listitem">
———. 2020b. <span>“<span class="nocase">Haddock (Melanogrammus aeglefinus) in divisions 7.b–k (southern Celtic Seas and English Channel)</span>,”</span> October. <a href="https://doi.org/10.17895/ices.advice.5897">https://doi.org/10.17895/ices.advice.5897</a>.
</div>
<div id="ref-ICES2021Cod028" class="csl-entry" role="listitem">
———. 2021. <span>“<span class="nocase">Benchmark Workshop on North Sea Stocks (WKNSEA)</span>,”</span> April. <a href="https://doi.org/10.17895/ices.pub.7922">https://doi.org/10.17895/ices.pub.7922</a>.
</div>
<div id="ref-ICESCodFactsheet" class="csl-entry" role="listitem">
———. n.d. <span>“ICES-FishMap Cod.”</span> <a href="https://www.ices.dk/about-ICES/projects/EU-RFP/EU%20Repository/ICES%20FIshMap/ICES%20FishMap%20species%20factsheet-cod.pdf">https://www.ices.dk/about-ICES/projects/EU-RFP/EU%20Repository/ICES%20FIshMap/ICES%20FishMap%20species%20factsheet-cod.pdf</a>.
</div>
<div id="ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" class="csl-entry" role="listitem">
Jones, Donald, Matthias Schonlau, and William Welch. 1998. <span>“Efficient Global Optimization of Expensive Black-Box Functions.”</span> <em>Journal of Global Optimization</em> 13: 455–92. <a href="https://doi.org/10.1023/A:1008306431147">https://doi.org/10.1023/A:1008306431147</a>.
</div>
<div id="ref-kodinariya2013review" class="csl-entry" role="listitem">
Kodinariya, Trupti M, Prashant R Makwana, et al. 2013. <span>“Review on Determining Number of Cluster in k-Means Clustering.”</span> <em>International Journal</em> 1 (6): 90–95.
</div>
<div id="ref-letham2018constrainedbayesianoptimizationnoisy" class="csl-entry" role="listitem">
Letham, Benjamin, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. 2018. <span>“Constrained Bayesian Optimization with Noisy Experiments.”</span> <a href="https://arxiv.org/abs/1706.07094">https://arxiv.org/abs/1706.07094</a>.
</div>
<div id="ref-MixMEwiki" class="csl-entry" role="listitem">
Pace, Matthew. 2024. <span>“MixME Wiki.”</span> <a href="https://github.com/CefasRepRes/MixME/wiki">https://github.com/CefasRepRes/MixME/wiki</a>.
</div>
<div id="ref-MixME" class="csl-entry" role="listitem">
Pace, Matthew, José Oliveira, Simon Fischer, and Paul Dolder. 2025a. <span>“MixME: An r Package to Simulation‐test Fisheries Management Robustness to Mixed‐fisheries Interactions.”</span> <em>Methods in Ecology and Evolution</em> 16 (February): 698–706. <a href="https://doi.org/10.1111/2041-210X.70005">https://doi.org/10.1111/2041-210X.70005</a>.
</div>
<div id="ref-MixMESupp" class="csl-entry" role="listitem">
———. 2025b. <span>“"Supplementary Material a: Conditioning a Multi-Stock, Multi-Fleet Operating Model for Celtic Sea Cod, Haddock and Whiting",”</span> February. <a href="https://besjournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111%2F2041-210X.70005&amp;file=mee370005-sup-0001-Supinfo1.pdf">https://besjournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111%2F2041-210X.70005&amp;file=mee370005-sup-0001-Supinfo1.pdf</a>.
</div>
<div id="ref-kmeansdocumentation" class="csl-entry" role="listitem">
RDocumentation. 2025. <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans">https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans</a>.
</div>
<div id="ref-DiceKrigingDocumentation" class="csl-entry" role="listitem">
Roustant, Olivier. 2025. <a href="https://www.rdocumentation.org/packages/DiceKriging/versions/1.6.1">https://www.rdocumentation.org/packages/DiceKriging/versions/1.6.1</a>.
</div>
<div id="ref-Roustant2012DiceKrigingpaper" class="csl-entry" role="listitem">
Roustant, Olivier, David Ginsbourger, and Yves Deville. 2012a. <span>“DiceKriging, DiceOptim: Two r Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization.”</span> <em>Journal of Statistical Software</em> 51 (1): 1–55. <a href="https://doi.org/10.18637/jss.v051.i01">https://doi.org/10.18637/jss.v051.i01</a>.
</div>
<div id="ref-DiceKrigingPaper" class="csl-entry" role="listitem">
———. 2012b. <span>“DiceKriging, DiceOptim: Two r Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization.”</span> <em>Journal of Statistical Software</em> 51 (1): 1–55. <a href="https://doi.org/10.18637/jss.v051.i01">https://doi.org/10.18637/jss.v051.i01</a>.
</div>
<div id="ref-Originalpaper" class="csl-entry" role="listitem">
Spence, Michael A. 2025. <span>“Using History Matching to Speed up Management Strategy Evaluation Grid Searches.”</span> <em>Canadian Journal of Fisheries and Aquatic Sciences</em> 82: 1–14. <a href="https://doi.org/10.1139/cjfas-2024-0191">https://doi.org/10.1139/cjfas-2024-0191</a>.
</div>
<div id="ref-MixedFisheriesStatusandManagement2023" class="csl-entry" role="listitem">
Sun, Ming, Yunzhou Li, Lisa Suatoni, Alexander Kempf, Marc Taylor, Elizabeth Fulton, Cody Szuwalski, Maria Teresa Spedicato, and Yong Chen. 2023. <span>“Status and Management of Mixed Fisheries: A Global Synthesis.”</span> <em>Reviews in Fisheries Science &amp; Aquaculture</em> 31 (4): 458–82. <a href="https://doi.org/10.1080/23308249.2023.2213769">https://doi.org/10.1080/23308249.2023.2213769</a>.
</div>
<div id="ref-ULRICH201238" class="csl-entry" role="listitem">
Ulrich, Clara, Douglas C. K. Wilson, J. Rasmus Nielsen, Francois Bastardie, Stuart A. Reeves, Bo S. Andersen, and Ole R. Eigaard. 2012. <span>“Challenges and Opportunities for Fleet- and Métier-Based Approaches for Fisheries Management Under the European Common Fishery Policy.”</span> <em>Ocean &amp; Coastal Management</em> 70: 38–47. https://doi.org/<a href="https://doi.org/10.1016/j.ocecoaman.2012.06.002">https://doi.org/10.1016/j.ocecoaman.2012.06.002</a>.
</div>
<div id="ref-ungredda2022efficientcomputationknowledgegradient" class="csl-entry" role="listitem">
Ungredda, Juan, Michael Pearce, and Juergen Branke. 2022. <span>“Efficient Computation of the Knowledge Gradient for Bayesian Optimization.”</span> <a href="https://arxiv.org/abs/2209.15367">https://arxiv.org/abs/2209.15367</a>.
</div>
<div id="ref-williams2006gaussian" class="csl-entry" role="listitem">
Williams, Christopher KI, and Carl Edward Rasmussen. 2006. <em>Gaussian Processes for Machine Learning</em>. Vol. 2. 3. MIT press Cambridge, MA.
</div>
<div id="ref-VikingDocumentation" class="csl-entry" role="listitem">
York, University of. n.d. <span>“Viking Documentation.”</span> <a href="https://vikingdocs.york.ac.uk/index.html">https://vikingdocs.york.ac.uk/index.html</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>