---
title: "Explanation of Mathematical Methods Behind the First Half of the Project"
author: "Emma Bowen"
format: html 
editor: visual
bibliography: references.bib
---

```{r}
#| include: false

# case_study8.R
# Brief: History matching using Gaussian process emulators for a small grid of
#        management parameters (Ftarget and Btrigger). The script:
#  - loads a precomputed data table `dat` containing target fishing mortality
#    (`Ftrgt`), biomass trigger values (`Btrigger`), median long-term catch
#    estimates (`catch_median_long`) and a risk metric (`risk1_full`).
#  - defines small helper functions to rescale/unrescale parameters to [0,1]
#    (useful for GP fitting) and a tiny tolerance check.
#  - fits Gaussian process emulators (DiceKriging `km`) to model log-catch and
#    log-risk using a small initial design and iteratively proposes new
#    evaluation points (Rounds 1..7) where the emulator predicts good trade-offs
#    between high catch and acceptable risk (<0.05). Plots are produced to
#    visualise median predictions, exceedance probabilities and candidate
#    selection.

dat <- data.frame(
  Ftrgt =c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.11,0.11,0.11,0.11,0.11,0.11,0.11,0.11,0.11,0.11,0.11,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.13,0.13,0.13,0.13,0.13,0.13,0.13,0.13,0.13,0.13,0.13,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.17,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.21,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.23,0.23,0.23,0.23,0.23,0.23,0.23,0.23,0.23,0.23,0.23,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.27,0.27,0.27,0.27,0.27,0.27,0.27,0.27,0.27,0.27,0.27,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.29,0.29,0.29,0.29,0.29,0.29,0.29,0.29,0.29,0.29,0.29,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.31,0.31,0.31,0.31,0.31,0.31,0.31,0.31,0.31,0.31,0.31,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.37,0.37,0.37,0.37,0.37,0.37,0.37,0.37,0.37,0.37,0.37,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.39,0.39,0.39,0.39,0.39,0.39,0.39,0.39,0.39,0.39,0.39,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.41,0.41,0.41,0.41,0.41,0.41,0.41,0.41,0.41,0.41,0.41,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.43,0.43,0.43,0.43,0.43,0.43,0.43,0.43,0.43,0.43,0.43,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.45,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.47,0.47,0.47,0.47,0.47,0.47,0.47,0.47,0.47,0.47,0.47,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.49,0.49,0.49,0.49,0.49,0.49,0.49,0.49,0.49,0.49,0.49,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5),
  Btrigger =c(110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05,110000,120000,130000,140000,150000,160000,170000,180000,190000,210000,2e+05),
  catch_median_long =c(34965,34973,34989.5,34990.5,35005,35005.5,35015,35019,35047.5,35046,35026.5,36976.5,36973.5,36984.5,36974,37000.5,37015,37015,37018.5,37004.5,37020.5,37014.5,38775.5,38786.5,38799,38804.5,38803.5,38826.5,38822.5,38824.5,38848,38839,38847.5,40406,40387,40424.5,40424.5,40437.5,40453.5,40465.5,40473.5,40481.5,40466.5,40485.5,41900,41916.5,41901.5,41904,41918.5,41927,41924,41929.5,41937.5,41944,41965.5,43249,43262,43263,43258,43276.5,43278.5,43293.5,43288.5,43284,43296,43303,44441,44448,44468.5,44474,44469,44476.5,44489.5,44499,44511.5,44514,44509.5,45502,45492.5,45539,45532,45532,45559,45577,45602.5,45602,45638.5,45621.5,46449,46475.5,46498.5,46495.5,46502.5,46521.5,46534,46570.5,46592.5,46616,46614,47329.5,47330,47353.5,47361,47406.5,47401,47413,47450.5,47454.5,47504,47477,48110.5,48139.5,48121,48138.5,48166.5,48168.5,48189,48197.5,48232,48294,48251.5,48816,48826,48827,48851.5,48864.5,48885.5,48899.5,48928,48943,49010.5,48981.5,49424,49426,49475.5,49480,49508.5,49512.5,49551,49563.5,49591,49707.5,49621.5,50010.5,50026.5,50053,50045,50074.5,50077,50104.5,50127,50168.5,50294,50241.5,50516,50512,50524,50521.5,50552.5,50586.5,50605,50648,50675,50884.5,50764.5,50907.5,50928.5,50923,50945,50961.5,51000.5,51039.5,51067,51160,51422,51264,51248.5,51245.5,51281.5,51322,51304,51358,51409,51470.5,51608,51973.5,51746,51582.5,51585,51600,51604.5,51607,51650.5,51765.5,51868,52010.5,52472.5,52272.5,51851,51821,51837.5,51857.5,51896.5,51927.5,52033.5,52219.5,52497,52888.5,52702.5,52057,52051.5,52052.5,52111,52119,52156,52348.5,52657.5,52891,53250,53097.5,52231.5,52247,52263,52306,52357,52515,52734,53027,53254,53558,53496,52389,52414,52462.5,52540.5,52609.5,52813,53066.5,53354,53671.5,53717.5,53801.5,52523.5,52585,52619,52691,52820.5,53134.5,53399,53699,53919,53687,53941,52642.5,52705,52737.5,52871,53118.5,53363,53668,54009.5,54090,53593.5,53977,52744,52803,52877.5,53062.5,53300.5,53619.5,53987.5,54232,54228.5,53419,53952,52793.5,52875.5,53004,53258.5,53534,53909.5,54241.5,54371,54177,53122,53795,52888.5,52972,53227.5,53430.5,53789.5,54167.5,54434.5,54372.5,54124.5,52846,53447.5,52927.5,53096.5,53306,53642,54036,54411,54563,54344,53819.5,52607,53170.5,52978.5,53158,53496.5,53883.5,54324.5,54572.5,54596.5,54219,53546.5,52452.5,52930,53011.5,53303.5,53629,54083.5,54523,54736,54448.5,54037,53236,52258,52705,53086.5,53440,53809,54358.5,54728,54774,54374.5,53781.5,53087.5,52109,52514.5,53224,53548,54034,54553.5,54865,54638,54099,53511,52836.5,52007,52371.5,53258.5,53672,54240.5,54704.5,54831,54494.5,53881,53206,52633,51814.5,52251.5,53336,53804.5,54406,54778.5,54735.5,54321.5,53657.5,52993,52475,51750,52086,53374,53947.5,54517.5,54838,54621.5,54069.5,53362.5,52779,52283.5,51672,51870,53457.5,54079,54628,54775,54426,53850,53073.5,52559,52126,51534.5,51776.5,53527.5,54255.5,54685,54695.5,54206.5,53605.5,52829,52348,51928.5,51483.5,51650.5,53691,54390.5,54735.5,54504,54035,53248,52578.5,52115,51779.5,51388.5,51488.5,53901,54507.5,54640,54320,53624.5,52873,52265.5,51886,51583,51219,51340,53935,54551,54509,54191.5,53215,52642,52084,51839,51514,51077.5,51204.5,54004,54454,54412,53796.5,52951,52420.5,51899.5,51653.5,51298,50914.5,51074),
  risk1_full =c(0.01035,0.01025,0.01015,0.01015,0.0101,0.01005,0.01005,0.01,0.00995,0.0099,0.0099,0.0104,0.01035,0.01035,0.01015,0.01015,0.0101,0.0101,0.01005,0.01005,0.00995,0.01,0.01065,0.0105,0.01035,0.01035,0.0102,0.01015,0.0101,0.0101,0.01005,0.01005,0.01005,0.01075,0.01065,0.0105,0.01035,0.01035,0.01025,0.01015,0.01015,0.0101,0.01005,0.0101,0.01095,0.0108,0.01065,0.0105,0.01035,0.01035,0.01025,0.01015,0.01015,0.0101,0.0101,0.01125,0.01105,0.01075,0.01065,0.0105,0.0104,0.01035,0.01025,0.0102,0.01015,0.01015,0.01145,0.0113,0.01095,0.01075,0.01065,0.0105,0.0104,0.01035,0.01025,0.01015,0.0102,0.01165,0.0115,0.0112,0.0109,0.01075,0.01065,0.0105,0.0104,0.01035,0.01025,0.01035,0.0118,0.0117,0.01135,0.0111,0.0108,0.01075,0.01065,0.0105,0.0104,0.01035,0.01035,0.012,0.0118,0.0117,0.01135,0.01105,0.0108,0.0107,0.01065,0.0105,0.01035,0.0104,0.0123,0.012,0.0117,0.0116,0.0113,0.01105,0.0108,0.0107,0.01065,0.0104,0.0105,0.0125,0.0123,0.01185,0.0117,0.0114,0.0112,0.01095,0.0108,0.01065,0.0105,0.01065,0.0128,0.0124,0.012,0.0118,0.0117,0.01135,0.01115,0.01095,0.0108,0.01065,0.01065,0.01335,0.0128,0.0124,0.01195,0.01175,0.01165,0.0114,0.0111,0.01095,0.0107,0.0108,0.01375,0.01305,0.0126,0.01215,0.0119,0.01175,0.01155,0.01135,0.0111,0.0108,0.01085,0.01435,0.01365,0.013,0.01255,0.0121,0.0118,0.01175,0.01145,0.0113,0.01085,0.0111,0.01505,0.01435,0.01345,0.0129,0.0126,0.0122,0.01195,0.0118,0.01145,0.0111,0.0112,0.01595,0.01535,0.0144,0.01365,0.01315,0.01265,0.01225,0.0121,0.01185,0.0113,0.01155,0.01825,0.01695,0.0157,0.0146,0.01395,0.01345,0.013,0.01245,0.0122,0.0115,0.01175,0.02055,0.0187,0.0174,0.0162,0.01515,0.0142,0.0136,0.01325,0.0126,0.01175,0.01225,0.023,0.02145,0.0195,0.0183,0.0166,0.01555,0.0147,0.01365,0.0132,0.0122,0.0126,0.02655,0.0243,0.02165,0.02035,0.01905,0.0173,0.0159,0.0149,0.01365,0.0127,0.0131,0.03135,0.02835,0.0257,0.0232,0.02135,0.01945,0.0173,0.0162,0.01445,0.01305,0.01375,0.03595,0.03275,0.02965,0.0266,0.02455,0.0224,0.02,0.01775,0.01615,0.01385,0.0143,0.04305,0.0394,0.0356,0.03165,0.0288,0.02615,0.02275,0.02035,0.01795,0.0144,0.01565,0.05125,0.04695,0.04195,0.03715,0.0325,0.02935,0.0263,0.02325,0.02055,0.01565,0.0178,0.06055,0.0556,0.0498,0.04385,0.0387,0.0333,0.0299,0.02595,0.0229,0.0176,0.0201,0.0693,0.0646,0.05805,0.05135,0.04465,0.0387,0.03355,0.03,0.02545,0.0194,0.0221,0.0838,0.07655,0.06795,0.0602,0.05215,0.0439,0.03785,0.03375,0.02885,0.02175,0.0249,0.098,0.08945,0.07985,0.06965,0.061,0.05115,0.0439,0.0381,0.0328,0.0244,0.02835,0.1138,0.10325,0.0922,0.08155,0.0709,0.0597,0.05085,0.0434,0.0374,0.02765,0.03245,0.1311,0.11815,0.1061,0.09305,0.08055,0.0686,0.05815,0.0497,0.04275,0.03125,0.03625,0.1511,0.1359,0.12035,0.10545,0.0924,0.07835,0.06595,0.05555,0.0471,0.0347,0.0406,0.1708,0.15385,0.1352,0.1193,0.1038,0.0897,0.07535,0.0632,0.0526,0.0394,0.04605,0.1919,0.17235,0.1528,0.13285,0.11585,0.1,0.08535,0.07135,0.0599,0.0438,0.05095,0.21485,0.19175,0.16985,0.1475,0.1277,0.11085,0.09605,0.08165,0.068,0.049,0.05735,0.23525,0.21115,0.1869,0.1634,0.1423,0.12235,0.10575,0.09195,0.07625,0.05485,0.06385,0.25645,0.22975,0.2028,0.1787,0.1558,0.13495,0.1151,0.1004,0.08545,0.0613,0.072,0.2779,0.25085,0.22095,0.19525,0.17,0.1477,0.12705,0.1107,0.09435,0.06765,0.0799,0.30175,0.27355,0.24105,0.2126,0.18495,0.1607,0.1399,0.1203,0.10375,0.0743,0.088,0.3241,0.2949,0.26,0.2288,0.2007,0.1747,0.1507,0.1303,0.1143,0.08145,0.0981)
)

rescale_Her <- function(runs,dat){
  # Rescale Ftarget and Btrigger in `runs` to [0,1] using ranges from `dat`.
  # This makes optimisation / GP fitting numerically more stable.
  ret <- runs
  minF <- min(dat$Ftarget)
  rangeF <- diff(range(dat$Ftarget))
  ret$Ftarget <- (ret$Ftarget - minF)/rangeF
  minB <- min(dat$Btrigger)
  rangeB <- diff(range(dat$Btrigger))
  ret$Btrigger <- (ret$Btrigger - minB)/rangeB
  return(ret)
}

unrescale_Her <- function(runs,dat){
  # Undo the rescaling performed by `rescale_Her` to recover original units.
  ret <- runs
  minF <- min(dat$Ftarget)
  rangeF <- diff(range(dat$Ftarget))
  ret$Ftarget <- ret$Ftarget*rangeF + minF
  minB <- min(dat$Btrigger)
  rangeB <- diff(range(dat$Btrigger))
  ret$Btrigger <- ret$Btrigger*rangeB + minB
  return(ret)
}

equal_tol <- function(x,y,tol=1e-12){
  # Numeric equality check with a tiny tolerance to avoid floating point issues.
  abs(x-y) < tol
}

library(DiceKriging)
library(DiceView)
library(dplyr)
library(plot3D)

# Use a logarithmic y-axis for the plot, i.e. log="y" tells the plot to display the y-axis on a log scale.

#CoPilot suggested this could result in a double transformation in lines 62 and 63, which may not be what is intended.
plot(dat$Ftrgt,log(dat$catch_median_long),log="y")
plot(dat$Btrigger,log(dat$catch_median_long),log="y")
plot(dat$Ftrgt,dat$risk1_full,log="y")
abline(h=0.05,col="red")
plot(dat$Btrigger,dat$risk1_full,log="y")
abline(h=0.05,col="red")


# Checks length of Ftrgt and Btrigger sets
length(unique(dat$Ftrgt))
length(unique(dat$Btrigger))

# Set Random Number Generator seed for reproducible random sampling.

# Copilot says I could change this to explore sensitivity of the process to the intial sample
set.seed(18)

#Round 1

#A space filling algorithm (seems simply chooses evenly spaced points) is used in the first round below to select points,
# as detailed in Section 3 Case Study in the paper
round1 <- data.frame(Ftrgt=sample(unique(dat$Ftrgt)[floor(seq(2,40,l=8))]),Btrigger=sample(unique(dat$Btrigger),size = 8))
plot(round1[,2:1])


# Choose an initial small set of design points and fit GP emulators for log-catch and log-risk using these initial runs.
Ftarget <- sort(unique(dat$Ftrgt))
Btrigger <- sort(unique(dat$Btrigger))
dat1 <- data.frame(expand.grid(Ftarget,Btrigger))
names(dat1) <- c("Ftarget","Btrigger")

gridd <- dat1[,c("Ftarget","Btrigger")]
gridd <- rescale_Her(gridd[order(gridd$Ftarget,gridd$Btrigger),],dat=dat1)

#CoPilpt says that runs only contains the data for the points evaluated in round 1
dat_run <- left_join(round1,dat)
names(dat_run) <- c("Ftarget","Btrigger","C_long","risk3_long")
runs <- rescale_Her(dat_run,dat=dat1)

# build the emulator for median catch

# res_cat is the log of observed median catch at the design runs. The GP
# on line 109 models this response using polynomial terms in the formula.
res_cat <- log(runs$C_long)

# Looks like it is using maximum likelihood estimation below and mentions this nugget thing which Mike was talking about
# CoPilot says "nugget: a tiny diagonal noise term for numerical stability."

# CoPilot also said "covtype = "exp": exponential correlation function (gives a less smooth prior than squaredâ€‘exponential)."
# which could maybe be something to follow up on

# Also has covariance type which is important for Gaussian Processes

# Below carries out the Kriging process for the two separate emulators, as detailed in the paper section 3.1
gp_cat <- km(~.^2,design=runs[,c("Ftarget","Btrigger")],estim.method="MLE",response = res_cat,nugget=1e-12*var(res_cat),covtype = "exp")

res_risk <- log(runs$risk3_long)

gp_risk <- km(~.^2,design=runs[,c("Ftarget","Btrigger")],estim.method="MLE",response = res_risk,nugget=1e-12*var(res_risk),covtype = "exp")

# Gets the Gaussian Process to produce a prediciton for risk at every point in gridd 
# gridd is the rescaled grid of Ftrgt and Btrigger
#This has a mean and standard deviation as we are unsure of the exact risk
pred_risk1_g <- predict(gp_risk,newdata=gridd,type="SK")
pred_cat1_g <- predict(gp_cat,newdata=gridd,type="SK")


# Estimate median predicted risk
med_risk1 <- exp(pred_risk1_g$mean)
# plot it
image2D(matrix(med_risk1,nrow=11),breaks=c(0,0.01,0.025,0.05,0.1,0.2,0.4),y=sort(unique(dat$Ftrgt)),x=sort(unique(dat$Btrigger)),xlab="Btrigger",ylab="Ftrgt")

# now lets look at the probability that the risk is less than or equal to 0.05
prisk1 <- pnorm(log(0.05),pred_risk1_g$mean,pred_risk1_g$sd+1e-12)
# plot it
image2D(matrix(prisk1,nrow=11),y=sort(unique(dat$Ftrgt)),x=sort(unique(dat$Btrigger)),xlab="Btrigger",ylab="Ftrgt",breaks=c(-1e-12,0.0001,0.05,0.5,0.9,1))


# Best catch observed so far where risk is below 0.05
max1 <- max(runs$C_long[runs$risk3_long < 0.05])

#Convert to max1 log scale as emulator trained on log(catch)
# pcat1 is the probability that the catch is less than or equal to max1 (the best safe catch observed) at each point in gridd
pcat1 <- pnorm(log(max1),pred_cat1_g$mean,pred_cat1_g$sd+1e-12)

# mark which points in gridd are still good candidates
eps <- 1e-4
possible1 <- (apply(cbind((1-pcat1) , prisk1),1,min) >  eps)

# getting catch out of log(catch) for each point in gridd
med_cat1 <- exp(pred_cat1_g$mean)
```

# Introduction

What mathematical methods will I explain in this document?

-   Gaussian processes

-   Bayesian History Matching

-   Kriging

-   Expected Improvement

-   Augmented Expected Improvement

-   Knowledge Gradient

-   K-Means Clustering

# Context

We are building on the paper "Using history matching to speed up management strategy evaluation grid searches". This paper is looking to find the Harvest Control Rule parametrised by $F_{target}$ and $B_{trigger}$ that maximises the median long-term catch whilst keeping the risk below 0.05 for a single-stock fishery. The paper does not consider fleet dynamics. We have an objective function in the paper which combines the risk and the catch and so this is the function we want to maximise to get our maximal median catch with the constraint of keeping the risk below 0.05.

To do this, the paper takes a Bayesian History Matching (BHM) approach. Firstly, we sample our first eight points which are spaced evenly throughout the sample space to get some initial data. Then, for each round we do the following:

-   We set up or update the Gaussian Process (GP) to model the risk and the GP to model the catch

-   We can then use the risk as a threshold so that we only consider the values of $F_{target}$ and $B_{trigger}$ that have risk below 0.05

-   We use the GP which is modelling the median catch to get the value for the median catch at every point in the sample space, which will have some uncertainty

-   We use BHM to remove any points that are implausible (that have a low probability of being higher than the current best median catch)

-   We select 8 plausible points to sample in the next round

We repeat this process until there is only one plausible point left and then we will accept this as being the $F_{target}$ and $B_{trigger}$ that maximise the median catch whilst keeping the risk below 0.05.

Now, we will look at the mathematics behind the methods above and also at the acquisition functions of Expected Improvement, Augmented Expected Improvement and Knowledge Gradient which I added to the original code.

# General theory

## Gaussian Processes

A Gaussian Process $F$ has a mean function $\mu_0$ and a covariance function $\operatorname{cov}_0(x_i,x_j)$. We can then evaluate the covariance function $\operatorname{cov}_0(x_i,x_j)$ for every pair $x_i,x_j$ where $i,j\in\{1, ..., n\}$ to find the covariance matrix $\Sigma_{1:n}$ . Then, $F$ is a probability distribution over our objective function $f$ with the property that, for any given collection of points ${x_1,...x_n}$, the marginal probability distribution on $F(x_{1:n}) = (F(x_1),...,F(x_n))$ is given by [@BayesianOptimisationTutorial]:

$$
F(x_{1:n}) \sim N((\mu_0(x_{1:n}),\Sigma_{1:n})
$$ {#eq-MultivariateNormalDist}

where

$$
\mu_0(x_{1:n}) = (\mu_0(x_1), . . ., \mu_0(x_n))
$$

We choose a covariance function such that inputs that have nearby points that have been evaluated have a more certain output than points that are further away from the points that have been evaluated [@Originalpaper]. This is equivalent to saying that if for some $x,x',x''$ in the design space we have $\|x - x'\| < \|x -x''\|$ for some norm $\| \cdot \|$, then $\operatorname{cov}_0(x, x') > \operatorname{cov}_0(x, x'')$ [@BayesianOptimisationTutorial].

We use a GP to emulate the objective function because it is much cheaper to evaluate than our objective function. We can calculate $F(x)$ for any $x$ in the design space as our estimate of $f(x)$ based on our current beliefs. This is true even for the evaluated points $x_1,...,x_m$ as the emulator is fitted to these points [@Originalpaper].

## Maximum Likelihood Estimation

When using GPs to emulate our objective function, we need to be able to estimate the coefficients of the objective function using the data we gain from evaluating our points $x_1,...,x_n$ [@BayesianOptimisationTutorial].

We can do this as follows. Firstly, we let the vector $\eta$ represent the hyperparameters that give us $\mu_0$ and $\operatorname{cov}_0$. Then, given the observations $f(x_{1:n}) = (f(x_1),...,f(x_n))$, we calculate the likelihood of these observations under the prior given $\eta$ which is denoted as $p(f(x_{1:n})|\eta)$ and modelled by @eq-MultivariateNormalDist . Lastly, we set $\hat{\eta}$ to the value that maximizes this likelihood @BayesianOptimisationTutorial\]:

$$
 \hat{\eta} = argmax_\eta p(f(x_{1:n})|\eta)
$$

## Kriging

Kriging is a Bayesian statistical method for modelling functions [@BayesianOptimisationTutorial]. Again, let $f$ be the objective function and we focus on the design space $X := \{x_1,...,x_n\}$. Now, if we have evaluated $n$ points such that we have $f(x_{1:n})$ and want to evaluate $x_{n+1}$ we let $k = n+1$ in @eq-MultivariateNormalDist . Then, we can compute the conditional distribution of $F(x_{n+1})$ given $f(x_{1:n})$ using Bayes' rule:

$$
F(x_{n+1})|f(x_{1:n}) \sim N(\mu_n(x_{n+1}), \sigma^2_n(x_{n+1}))
$$ {#eq-posteriordistributiongivensamples}

where:

$$
mu_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}(f(x_{1:n}) - \mu_0(x_{1:n}))  + \mu_0(x_{n+1})
$$

$$
\sigma^2_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1},x_{n+1}) - \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}\operatorname{cov}_0(x_{1:n},x_{n+1})
$$

where:

$$
\operatorname{cov}_0(x_{n+1}, x_{1:n}) = (\operatorname{cov}_0(x_{n+1}, x_1), ... , \operatorname{cov}_0(x_{n+1}, x_n))
$$

This conditional distribution $F(x_{n+1})|f(x_{1:n})$ is called the posterior probability distribution for $x_{n+1}$. We can calculate this distribution for every point in the design space $X$. This results in a new GP $F_n$ with a mean vector and covariance kernel that depend on the location of the unevaluated points, the locations of the evaluated points $x_{1:n}$, and their values $f(x_{1:n})$[@BayesianOptimisationTutorial]. So, we can update our GP every round based on the new points we have evaluated.

## Bayesian History Matching

Let $x$ be a point in the sample space. We begin with some uncertainty about our objective function $f(x)$ [@Originalpaper]. However, we can make probabilistic statements such as:

\begin{equation}
    P(f(x)>a)= \int_{a}^{\infty}P(f(x))df(x)
\end{equation}

Once we evaluate another point $x'$ where $x' \neq x$, we are able to use Bayes' Theorem improve our integral to:

\begin{equation}
    P(f(x)>a|f(x'))= \int_{a}^{\infty}P(f(x)|f(x'))df(x)
\end{equation}

We now let $a=max\{f(x_1),...,f(x_n)\}$ where $n$ is the number of points we have evaluated so far. For the first round, $n=8$ but as the rounds increase we make sure to include all previous points of the objective function that have been evaluated.[@Originalpaper] We remove the point $x$ if:

$$
P(f(x)>a|f(x_{1:n}) = \int_{a}^{\infty}P(f(x)|f(x_{1:n})df(x) <\varepsilon
$$ {#eq-removeimplausiblepoint}

for a small $\varepsilon > 0$ until no plausible points remain. Then, the optimum will be $x^*$ such that:

$$
f(x^*) = max\{f(x_{1:n})\}
$$

as our index $n$ counts the number of points we have evaluated throughout the whole simulation.

## Expected Improvement

The first type of acquisition function we will look at is Expected Improvement (EI). Suppose we have sampled the points $x_1, ... ,x_n$ and observe the values $f(x_{1:n})$. Then, if we were to return a solution at this point, bearing in mind we observe the objective function $f$ without noise and we can only return points we have already evaluated, we would return $f^*_n = max\{f(x_1),...,f(x_n)\}$ [@BayesianOptimisationTutorial]. Imagine we then consider evaluating another point $x_{n+1}$ to get $f(x_{n+1})$. We can then define the Expected Improvement as:

\begin{equation}
    EI_n(x_{n+1}) := E[F(x_{n+1})|f(x_{1:n})-f^*_n]^+
\end{equation}

where $[F(x_{n+1})-f^*_n]^+$ is the positive part of $[F(x_{n+1})-f^*_n]$. This acquisition function is relatively easy to optimise and many different methods have been developed for doing this [@BayesianOptimisationTutorial].

There is another expression for $EI_n(x_{n+1})$:

$$
EI_n(x_{n+1}) =  [(\mu_n(x_{n+1})-f_n^*\cdot\Phi(Z))+(\sigma_n(x_{n+1})\cdot\phi(Z))]^+
$$ {#eq-EIincode}

where again the notation $[\cdot]^+$ means the positive part and where:

$$
Z = \frac{\mu_n(x_{n+1})-f_n^*}{\sigma_n(x_{n+1})}
$$

@eq-EIincode can be gained from @eq-MultivariateNormalDist by setting $k = n+1$ and then studying the distribution of $F(x_{n+1})-f^*_n$. However, we can also consider it as a version of Equation (15) from [@EfficientGlobalOptimizationofExpensiveBlackBoxFunctions] where we first flip the signs as we are focused on the maximisation case and then set $f_{min} = f^*_n$, $\hat{y} = \mu_n(x)$ and $s = \sigma_n(x)$.

## Augmented Expected Improvement

This was included to help make the method perform better for noisy functions which will make it more generally applicable [@GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels]. To deal with these noisy observations, a change was proposed to standard EI function as detailed below. This change seems mostly to have been justified by empirical performance [@letham2018constrainedbayesianoptimizationnoisy].

By adjusting Equation (12) found in [@GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels] to our own notation, we get that:

\begin{equation}
    AEI_n(x_{n+1})=E_n[F(x_{n+1})|f(x_{1:n})-f^*_{eb}]^+\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\end{equation}

where $\sigma_{obs}$ is the standard deviation of the noise variable set by the user and $\sigma_n(x)$ is the standard deviation of GP $F$ at the $n^{th}$ iteration, as used beforehand. We have also changed $f^*_n$ to $f^*_{eb}$ which is the highest predicted mean at any sampled point so far so that we take into account that the uncertainty in our observations could cause a large spike [@GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels].

## Knowledge Gradient

We remove the assumption of EI that we have to return a pre-evaluated point as our best point [@BayesianOptimisationTutorial]. This allows us to do some different computations to the ones in EI. We also now start by saying that the solution we would choose if we have to stop sampling after n points would be the point in the design space with the largest $\mu_n(\cdot)$ value, where $\mu_n(\cdot)$ is the mean vector of the posterior probability distribution after $n$ iterations. We call this maximum ${x_n^*}$ and then can say that $F(x_n^*)$ is random under the posterior distribution and has the mean vector after sampling $f(x_{1:n})$ of:

\begin{equation}
    \mu_n^* := \mu_n(x_n^*) = max_{x}\mu_n(x)
\end{equation}

where $x$ is any point in the sample space [@BayesianOptimisationTutorial].

Then, we imagine that we are now allowed to sample a new point $x_{n+1}$. We get a new posterior distribution at the point $x$ which we can calculate using @eq-posteriordistributiongivensamples by replacing $x_{n+1}$ with $x$ and $x_{1:n}$ with $x_{1:n+1}$ to include our new observation. This will have the posterior mean function $\mu_{n+1}(\cdot)$ and the conditional expected value for $F(x_n^*)$ changes to be:

\begin{equation}
    \mu_{n+1}^* := max_x\mu_{n+1}(x)
\end{equation}

So, we can see that the increase in the conditional expected value of $F(x_n^*)$ by sampling the new point $x_{n+1}$ is [@BayesianOptimisationTutorial]:

\begin{equation}
    \mu_{n+1}^* - \mu_n^*
\end{equation}

While this quantity is unknown before we sample $x_{n+1}$ we can calculate it's expected value given our observations $x_1,...,x_n$. The Knowledge Gradient for sampling at a new point $x$ in the design space is defined as [@BayesianOptimisationTutorial]:

$$
KG_n(x) := E_n[\mu_{n+1}^* - \mu_n^*|x_{n+1} = x]
$$ {#eq-KnowledgeGradient}

where again $E_n$ indicates the expectation taken under the posterior distribution at the $n^{th}$ iteration. We would sample the point $x$ with the largest $KG_n(x)$ as our next point [@BayesianOptimisationTutorial].

The easiest way to calculate the KG is via simulation. This can be done by simulating one possible value for $f(x_{n+1})$. Then, we calculate $\mu_{n+1}^*$ and subtract $\mu_n^*$. We iterate this process many times so that we can find the average of $\mu_{n+1}^* - \mu_n^*$ and this allows us to estimate $KG_n(x)$ [@BayesianOptimisationTutorial]. This process, or calculating @eq-KnowledgeGradient directly from the properties of the normal distribution, both work well in discrete, low dimensional problems which is the situation we are in for the first half of the project [@BayesianOptimisationTutorial].

Alternatively, we can calculate $\mu_{n+1}$ using the formula below [@ungredda2022efficientcomputationknowledgegradient]:

$$
\mu_{n+1}(x) = \mu_n(x) + \frac{\operatorname{cov}_n(x_{n+1},x)}{\operatorname{var}_n(x_{n+1}) + \sigma_{\mathrm{obs}}^2}(F(x_{n+1})-\mu_n(x_{n+1}))
$$ {#eq-updatemuKG}

where $\sigma_{obs}$ is a noise variable which can be determined by the user [@ungredda2022efficientcomputationknowledgegradient]. From the GP for median catch, we get $\operatorname{cov}_n(x_{n+1},x)$ and the standard deviation $\sigma_{\mathrm{obs}}^2$.

## Kmeans process for selecting multiple points

Combining a clustering method with an acquisition function was an idea I had early on in the project. I was then able to find literature on the subject, including using kmeans clustering.

We have now been able to determine which points are possible based on the probability that their median catch is higher than the current maximum median catch (using Bayesian History Matching) and the probability that their risk is less than 0.05. These points will be called the Possible Space, $PS$. We have then assigned a value to each point in $PS$ using one of the acquisition functions above. Now, we want to decide which 8 points are best to evaluate next.

For sampling only one point next, this would be very simple as you would take the point with the highest value assigned by the acquisition function [@BayesianOptimisationTutorial]. However, we want to sample 8 points next so that we continue the pattern set up in the original paper and we want a good trade-off between exploration and exploitation [@Originalpaper],[@batchspreadinoutjustification].

So, we use the `kmeans` function which is part of the stats package in R (which is automatically loaded into an R session). This function uses the algorithm from Hartigan and Wong, 1979 by default[@AKMeansClusteringAlgorithm],[@kmeansdocumentation]. The k-means clustering method is the most commonly used due to its simplicity compared to other clustering algorithms[@kodinariya2013review.]

This algorithm creates $k$ clusters (groups of points) such that the points within each cluster have the sum of squares to the centre of their cluster smaller than it would be to the centre of any other cluster [@kmeansdocumentation]. It starts by defining $k$ centroids which should be placed as much as possible far away from each other [@kodinariya2013review]. Then, we take each point in the space and associate it to the nearest centroid. We stop when every point has been assigned to a centroid [@kodinariya2013review]. At this point, we re-calculate $k$ new centroids as the centers of the clusters created by the previous step. This may result in some points changing clusters [@AKMeansClusteringAlgorithm]. We repeat this process until no points change clusters [@AKMeansClusteringAlgorithm],[@kodinariya2013review].

However, before the algorithm can start, we must specify how many clusters we want [@kodinariya2013review]. This can be difficult in many cases [@kodinariya2013review]. In our case, it is relatively simple as we know how many points we want to sample next and so we set this to be the number of clusters. We then run the algorithm on $PS$ to form the clusters. Then, we pick the point with the highest value of the acquisition function from each cluster to sample in our next round. This allows us to search for viable points by looking in the Possible Space but also to keep the points we are going to sample spread out so that we can balance exploitation and exploration more effectively [@batchspreadingoutjustification].

# Application of theory in my project

## Set up the Gaussian Processes

We are focusing on maximising the objective function from the paper which is given below [@Originalpaper]:

$$
f(\theta) = I_{[0.95,1]}(P(B(\theta)>B_{lim})) \times C(\theta)
$$

where $C(\theta)$ is median long term catch, $B(\theta)$ is long-term $SSB$ and:

$$
I_{[0.95,1]}(x) = 
\begin{cases}
1 & \text{if   } x\in[0.95,1] \\
0 & \text{otherwise }
\end{cases}
$$

is an indicator function.

To do this, Spence uses two GPs where the risk GP models $ln(risk)$ and the catch GP models $ln(median\ catch)$ [@Originalpaper]. Maintaining the notation from the Spence 2025 paper, we use $m_1$ for the mean function of the catch GP and $m_2$ for the mean function of the risk GP [@Originalpaper]:

$$
\begin{split}
m_1 (\phi) = \beta_{1,0} + \beta_{1,1} (\operatorname{ln}(\phi_1 + 0.1))+\beta_{1,2}(\operatorname{ln}(\phi_1 + 0.1))^2 + \\
 \beta_{1,3}(\operatorname{ln}(\phi_1 + 0.1))^3+ \beta_{1,4}(\phi_2\operatorname{ln}(\phi_1 + 0.1)) + \beta_{1,5}\phi_2
\end{split}
$$

$$
m_2(\phi) = \beta_{2,0} + \beta_{2,1}\phi_1 + \beta_{2,2}\phi_2 + \beta_{2,3}\phi_1\phi_2
$$

where all of the above $\beta_{s,t}$ for $s \in \{1,2\}$ and $t \in \{1,2,3,4,5\}$ are coefficients to be found through maximum likelihood estimation and:

$$
\phi_1 = \frac{F_{target}-0.1}{0.4} \quad \textrm{and} \quad \phi_2 = \frac{B_{trigger}-110000}{90000}
$$

where we rescale for numerical stability in the GP [@Originalpaper].

Our covariance function $c$ for both GPs is the variance $\sigma_i^2$ (which is acting as a scalar) times the Ornstein-Uhlenbeck correlation function [@Originalpaper]:

$$
r_i(\phi,\phi',\delta_i) = \operatorname{exp} \left(-\frac{|\phi_1 - \phi'_1|}{\delta_{i,1}} - \frac{|\phi_2 - \phi_2'|}{\delta_{i,2}}\right)
$$

where ${\delta_{i,1}}$ and ${\delta_{i,2}}$ are the length scales for each of the $\phi_1$ and $\phi_2$ terms respectively [@williams2006gaussian].

We need to sample our first round of eight points before setting up the GPs so that we have enough data to estimate all of the coefficients in our GPs [@EfficientGlobalOptimizationofExpensiveBlackBoxFunctions]. Note that until we have sampled sixteen points, we set the prior of the catch GP to be the same as the risk GP, $m_2(\phi)$, except with different coefficients [@Originalpaper]. This is because we need to estimate the coefficients for the mean function for the catch GP and the length scales for the covariance function $c$ from the same data [@EfficientGlobalOptimizationofExpensiveBlackBoxFunctions],[@Roustant2012DiceKrigingpaper]. For mathematical stability, these functions should have less than eight coefficients between them in our first round as we are only sampling eight points per round due to computational limitations encountered at the time of the Spence 2025 paper [@EfficientGlobalOptimizationofExpensiveBlackBoxFunctions],[@Originalpaper]. However, after our first round we reset the mean function for the catch GP to $m_1(\phi)$ [@Originalpaper].

We can set up Gaussian Processes (GPs) in R using the DiceKriging package as below:

``` r
gp_cat <- km(~.^2,design=runs[,c("Ftarget","Btrigger")],estim.method="MLE",
             response = res_cat,nugget=1e-12*var(res_cat),covtype = "exp")

gp_risk <- km(~.^2,design=runs[,c("Ftarget","Btrigger")],estim.method="MLE",
              response = res_risk,nugget=1e-12*var(res_risk),covtype = "exp")
```

where `covtype` tells us the type of covariance function we are using and `estim.method` tells us how to estimate unknown parameters [@DiceKrigingDocumentation]. Here, `nugget` is a small value being used to ensure we have an invertible matrix and telling the GP that our objective function is deterministic because it is so small [@DiceKrigingDocumentation]. If our objective function had noise, `nugget` would instead be used to add that noise along the diagonal of the covariance matrix to encode that noise into our GP [@mackay1998introduction].

The `design` variable gives the input parameters of the evaluated points we have so far [@DiceKrigingDocumentation]. Here, `estim.method` is set to `"MLE"` which means we are using the maximum likelihood estimate to get the hyperparameters of $m_1(\phi),m_2(\phi)$ and $c$ for our GPs [@DiceKrigingDocumentation],[@Roustant2012DiceKrigingpaper].

In the code, we predict the $ln(median\ catch)$ and $ln(risk)$ at every point in the design space using the code below:

``` r
pred_risk_g <- predict(gp_risk,newdata=gridd,type="SK")
pred_cat_g <- predict(gp_cat,newdata=gridd,type="SK")
```

We can then exponentiate these results where needed. We are building our GPs with log of the values we want because this helps us generate better predictions [@GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels].

We have been able to visualise this process using the code below. Firstly, we can look at the GP for risk after the first round of `case_study8.R`:

```{r}
library(DiceView)
library(rgl)

Xlim <- rbind(
  apply(gp_risk@X, 2, min),
  apply(gp_risk@X, 2, max)
)


sectionview3d(
  gp_risk,
  dim = 2,
  Xlim = Xlim,
  Xlab = c("Ftarget", "Btrigger"),
  ylab = "GP mean (log-risk)",
  npoints = c(100, 100),
  col = "lightblue",
  scale = TRUE,
  col_points = "red",
  add = FALSE
)

rgl::aspect3d("iso")
rgl::view3d(theta = 40, phi = 25, zoom = 0.8)

rglwidget()
```

The middle plane is the mean of the GP, whereas the bottom and top planes represent the lower and upper bounds of the 95% confidence interval respectively. The planes meet at the evaluated points, which are the red dots on the diagram. The scales are odd due to the re-scaling of $F_{target}$ and $B_{trigger}$ and fitting our GPs to log of the values we want throughout the code which helps to keep the GP stable [@GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels].

We can then also do this for the catch GP:

```{r}
library(DiceView)
library(rgl)

Xlim <- rbind(
  apply(gp_cat@X, 2, min),
  apply(gp_cat@X, 2, max)
)


sectionview3d(
  gp_cat,
  dim = 2,
  Xlim = Xlim,
  Xlab = c("Ftarget", "Btrigger"),
  ylab = "GP mean (log-catch)",
  npoints = c(100, 100),
  col = "lightblue",
  scale = TRUE,
  col_points = "red",
  add = FALSE
)

rgl::aspect3d("iso")
rgl::view3d(theta = 40, phi = 25, zoom = 0.8)

rglwidget()
```

## Excluding implausible points

Firstly, we enforce the precautionary threshold of $P(ln(risk)\le0.05)$ by calculating:

``` r
prisk <- pnorm(log(0.05),pred_risk_g$mean,pred_risk_g$sd+1e-12)
```

where we include `+1e-12` in the above to prevent the variance becoming negative due to imprecision when calculating `pred_risk_g$sd`. This would encounter an error from the `pnorm` function which would stop the code [@DiceKrigingDocumentation],[@Roustant2012DiceKrigingpaper].

We exclude any points not meeting the precautionary threshold by setting their value of the acquisition function to 0 so that they will not be chosen as a point to sample in the next round.

Then, Bayesian history matching speeds up the process by removing any points that are implausible according to @eq-removeimplausiblepoint. This process is written up in the code as the below, where we have set $\varepsilon = 0.0001$ [@Originalpaper]:

``` r
#Finds the current maximum catch which also has risk < 0.05
#Note that we have exponentiated for this part
current_max <- max(runs$C_long[runs$risk3_long < 0.05])

#Finds the probability that the log(catch)<= log(current_max) for 
#all points in the design space
pcat <- pnorm(log(current_max), pred_cat_g$mean, pred_cat_g$sd + 1e-12)

#Check which points have P(log(catch) >= log(curent_max)) > 0.00001)
#and have P(log(risk) <= log(0.05)) > 0.00001.
#These are our plausible points that remain.
possible <- (apply(cbind((1 - pcat), prisk), 1, min) > eps)
```

## Deciding on next point to sample

The method we use here depends on which script we are running. We have three options.

### Expected Improvement

We use the @eq-EIincode expression. This method of EI can be coded up as the function:

``` r
expected_improvement <- function(mu, sigma, y_best, xi = 0.05, task = "max", 
                                 pred_risk, eps = 1e-4) 
{
  #Setting up EI vector
  ei <- numeric(length(mu))
  #Determining points with P(risk<=0.05)>0.00001
  safe_points <- pred_risk >= eps

  # Only calculate EI if there are safe points and the task is a valid option
  if (any(safe_points)) {
    if (task == "min") 
        imp <- y_best - mu[safe_points] - xi
    if (task == "max") 
        imp <- mu[safe_points] - y_best - xi 
    else 
        stop('task must be "min" or "max"')
    
    Z <- imp / sigma[safe_points]
    #Only calculate EI for safe points
    ei[safe_points] <- imp * pnorm(Z) + sigma[safe_points] * dnorm(Z)
    ei[safe_points][sigma[safe_points] == 0.0] <- 0.0
  }

  return(ei)
}
```

where `mu` is the mean of the catch GP and `sigma` is the standard deviation of the catch GP. Then we also have `y_best` which is the current best $ln(median\ catch)$, `xi` which is a parameter balancing exploration and exploitation. We set `pred_risk = prisk` and `eps = 1e-4` and these are the same as before.

### Augmented Expected Improvement

As our situation has no noise, in our code we are still using $f^*_n$[@Originalpaper]. Thus, we code up the equation:

\begin{equation}
    AEI_n(x_{n+1})=EI_n(x_{n+1})\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\end{equation}

in the code below:

``` r
augmented_expected_improvement <- function(mu, sigma, y_best, xi = 0.05, 
                                           task = "max", pred_risk, eps = 1e-4,
                                           noise_var = 0) 
{
  ei <- numeric(length(mu))
  safe_points <- pred_risk >= eps

  # Only calculate AEI for safe points
  if (any(safe_points))
        if (task == "min") 
            imp <- y_best - mu[safe_points] - xi
        if (task == "max") 
            imp <- mu[safe_points] - y_best - xi 
        else 
            stop('task must be "min" or "max"')

        Z <- imp / sigma[safe_points]
        ei[safe_points] <- imp * pnorm(Z) + sigma[safe_points] * dnorm(Z)
        ei[safe_points][sigma[safe_points] == 0.0] <- 0.0
  
        # Augmentation factor to handle noise
        # When noise_var = 0 it reduces to standard EI
        augmentation_factor <- 1 - sqrt(noise_var / (noise_var + sigma^2))
        aei <- ei * augmentation_factor
  
  # Giving points with prob(risk <= 0.05) < 0.0001 an expected 
  # improvement of zero so we avoid them
  aei <- ifelse(pred_risk < eps, 0, aei)
  
  return(aei)
}
```

where `noise_var` corresponds to $\sigma_{obs}^2$ and `sigma` corresponds to $\sigma_n(\cdot)$ and the other arguments are as in EI.

### Knowledge Gradient

@eq-updatemuKG is mirrored in the code as:

``` r
# nsim number of simulated observations from a normal
y_sim <- rnorm(nsim, mu_i, sigma_i)
# the covariance between every point in the grid and the point x_i
cov_xp_xi <- cov_grid[, i]
# denominator term in the KG update formula
denom <- var[i] + obs_noise_var
# For each simulated y, compute updated max(mu)
max_after <- numeric(nsim)

for (s in seq_len(nsim)) {
                # says what would the new maximum mu be if we observed this
                # simulated value at the point x_i   
                # Evaluates the posterior mean for every point in the 
                # space, updating other points based on closeness 
                # to the evaluated point by using the covariance matrix
                mu_new <- mu + cov_xp_xi * (y_sim[s] - mu_i) / denom
                # takes this new maximum mu into a vector for later averaging
                max_after[s] <- max(mu_new)
            }
```

where `mu_new` is $\mu_{n+1}(x)$, `cov_xp_xi` is $\operatorname{cov}_n(x_{n+1},x)$, `y_sim[s]` is a simulated value drawn from $F_{n+1}$ and `mu_i` is $\mu_n(x_{n+1})$. We can also see that `var[i]` is $\operatorname{var}_n(x_{n+1})$ and `obs_noise_var` is $\sigma_{\mathrm{obs}}^2$ which makes the denominators equivalent.

Now, we have everything needed to compute @eq-KnowledgeGradient . We can write the whole process in code as below:

``` r
knowledge_gradient_sim <- function(mu, sigma, model, obs_noise_var = 0, 
                                   nsim = 100, pred_risk, eps = 1e-4) 
{ 
    X_pred <- dat[, c("Ftrgt", "Btrigger")]
    cov_grid <- cov_exp(X_pred, X_pred, theta = model@covariance@range.val, 
                        sigma2 = model@covariance@sd2)
    m <- length(mu)
    var <- sigma^2
    # Current best mean catch
    mu_best <- max(mu)   
    kg <- numeric(m)
    
        # Loop over all candidate points
        for (i in seq_len(m)) {
            # Set KG to 0 for points where risk is too high
            if (pred_risk[i] < eps) {
                kg[i] <- 0
                next
            }

            mu_i <- mu[i]
            sigma_i <- sqrt(var[i] + obs_noise_var)
            # nsim simulated observations
            y_sim <- rnorm(nsim, mu_i, sigma_i)
            # the covariance between every point in the grid and 
            # the point x_i
            cov_xp_xi <- cov_grid[, i]
            # denominator term in the KG update formula
            denom <- var[i] + obs_noise_var
            # For each simulated y, compute updated max(mu)
            max_after <- numeric(nsim)
            for (s in seq_len(nsim)) {
                # says what would the new maximum mu be if we observed this
                # simulated value at the point x_i by implementing 
                # the standard formula for this in GP posterior updating   
                # This evaluates the posterior mean for every point in the 
                # space, updating other points based on closeness 
                # to the evaluated point by using the cov matrix
                mu_new <- mu + cov_xp_xi * (y_sim[s] - mu_i) / denom
                # takes this new maximum mu into a vector for averaging
                max_after[s] <- max(mu_new)
            }
            # Computes expected increase in the maximum posterior mean
            kg[i] <- mean(max_after - mu_best)
        }
    # Returns the vector of KG values for each point in the design space
    kg
}
```

where the KG for each point is calculated by:

``` r
kg[i] <- mean(max_after - mu_best)
```

where `max_after` is $\mu_{n+1}^*$ and `mu_best` is $\mu_n^*$.

All the arguments of the function are as in EI with a few exceptions. `obs_noise_var` is the new name for the noise variable set by the user; `theta` is the length scale of our GP determining how quickly the objective function changes as $Ftarget$ and $Btrigger$ change; and `nsim` tells the code how many times to estimate the value of $f(x_{n+1})$ using $F(x_{n+1})$ before we calculate $KG_n(x)$[@DiceKrigingDocumentation].

### Kmeans process

To then make sure that the next points we sample are spread out across the sample space, we use the Kmeans process as coded up below [@batchspreadingoutjustification]:

``` r
# Select next points

# cand is our Possible Space
 if (nrow(cand) <= 8) {
    # Here, we take all of the points as there are less than the 
    # amount we want to sample left
   next_points <- cand[order(-cand$ei), c("Ftarget", "Btrigger")]
 } else {
    # We order the points based on the value given 
    # by the acquisition function, from highest to lowest here
   top_candidates <- cand[order(-cand$ei), ][1:nrow(cand), ]
   # We need to set a seed to make the points that kmeans
    # picks as the first centroids reproducible
   set.seed(123)
   # We find the clusters
   km_result <- kmeans(top_candidates[, c("Ftarget", "Btrigger")], centers = 8)
   # We add the cluster column to our table
   top_candidates$cluster <- km_result$cluster
   # We pick the point with the highest value from the 
   # acquisition function from each cluster and assemble these 
   # points into a vector
   next_points <- do.call(rbind, 
                  lapply(split(top_candidates, top_candidates$cluster),
                         function(df) {
     df[which.max(df$ei), c("Ftarget", "Btrigger", "ei")]
   }))
 }
```

Then, at the beginning of our next round we sample the eight points given in `next_points`.

## Updating our GPS

In our second round, we need to update our GPs with new data [@Originalpaper]. Our process for updating GPs can be seen in the code below:

``` r
gp_risk <- km(~.^2, design = runs[, c("Ftarget", "Btrigger")], estim.method = "MLE",
              response = res_risk, nugget = 1e-12 * var(res_risk), covtype = "exp")
```

This looks the same as before, but the `runs` variable includes all of our evaluated points and so we are adding the points that have been newly evaluated this round. Hence, we can do the calculations from the Kriging section to update the GP with a new mean vector and covariance kernel for our next round.

For the catch GP, we move on to using a new prior because by the time we create this GP we have sampled 16 points [@Originalpaper]. Here is the median catch GP for the second round and onwards:

``` r
gp_cat <- km(~I(log(Ftarget+0.1)^2)+I(log(Ftarget+0.1))+ I(log(Ftarget+0.1)^3) + 
                I(Btrigger) + I(Btrigger * log(Ftarget+0.1)),
                design=runs[,c("Ftarget","Btrigger")],estim.method="MLE",
                response = res_cat,nugget=1e-12*var(res_cat),covtype = "exp")
```

Now, we repeat the full process described in the Application of theory in my project section until there are no points with a $KG > 0$. Then, the optimal point is the $x^*$ that has the highest catch out of the precautionary points. In our case, $x^*$ is the $F_{target}$ and $B_{trigger}$ that will give the highest median long-term catch whilst following the precautionary principle [@Originalpaper].

### References