---
title: "Explanation of Mathematical Methods Behind the Second Half of the Project"
author: "Emma Bowen"
format: html 
editor: visual
bibliography: second_half_references.bib
---

# Introduction

What mathematical methods will I explain in this document that have not been explained in [Explanation of Mathematical Methods from First Half of the Project](https://github.com/dtj504-pixel/DissertationWork/blob/main/Explanation%20of%20Mathematical%20Methods%20From%20First%20Half%20of%20the%20Project.qmd)?

-   Calculating the risk

-   The basics of the MixME model - BREAK DOWN A BIT MORE LATER

-   Parallelisation

# Context

The aim of this half of the project is to apply the methods from the first half of the project to mixed fisheries using the MixME R package. I have used the methods from the first half of the project to write code that follows a very similar process, but where the aim is now to find the Ftarget for each stock that is precautionary but maximises total catch over the years 2030 to 2039. I set this goal because MixME is designed to run projections into the future, and looking at catch over all years of the projection will minimise the chance that a simulation where a stock fails is chosen [@MixME],[@MixMEwiki]. The 20 year projection from 2020-2039 was kept consistent with examples on the MixME documentation and follows guidelines from ICES to create long-term projections based on the biology of the stocks [@MixMEwiki],[@ICES2019WKGMSE2]. It also follows ICES guidelines to only calculate yield and risk for the last ten years for long-term projection, to allow time for a recovery period [@ICES2019WKGMSE2].

I focused on the datasets from the Fixed fishing mortality management strategy example (`mixedfishery_MixME_om`) and the Exploring simulation outputs example (`mixedfishery_MixME_input`) which are both in the MixME documentation [@MixMEwiki]. However, for the second dataset `mixedfishery_MixME_input` I was given a shortcut method by a colleague at Cefas which takes a more direct approach to the simulation and so is deterministic. The code for these datasets is `Optimising_ftarget_in_MixME_mult_points_parallel.R` and `Two_stocks_Optimising_ftarget_in_shortcut_model.R` respectively.

Both of these datasets have two stocks (cod and haddock) and two fleets [@MixMEwiki]. I was told by a colleague at Cefas that the stocks are North Sea cod and Celtic Sea haddock, but using citations I can only back up that they are Atlantic cod and haddock [@MixMEwiki],[@ICESCodFactsheet],[@ICESCodFactsheet]. This allows me to use the same methods as in the first half of the project by replacing $F_{target}$ with $F_{cod}$ and $B_{trigger}$ with $F_{had}$, where $F_{cod}$ and $F_{had}$ are the fishing mortalities for cod and haddock respectively.

For this half of the project, we have switched to modelling SSB directly instead of calculating the risk. This is due to the simulation being deterministic as both datasets only have one iteration and the noise for this is pre-calculated [@MixMESupp]. These conditions simplify the simulation but unfortunately mean that we cannot use the standard ICES definition of risk to calculate $Risk = P(SSB < B_{lim})$ [@MixMESupp],[@ICES2019WKGMSE2]. Instead, we extract $\textrm{min}(SSB)$ for each sampled point from the years 2030-2039 in the simulation and then model these as a GP [@Originalpaper][@ICES2019WKGMSE2]. This allows us to predict possible $\textrm{min}(SSB)$ values at each point in the design space and see how many of these predictions fall below $B_{lim}$ to calculate $P(\textrm{min}(SSB) < B_{lim})$ [@Originalpaper]. We do this for each stock. This method of calculation satisfies the ICES precautionary standard [@ICES2019WKGMSE2].

Due to the change in stocks being modelled, I have decided to keep the GP prior for catch modelled by the `~.^2` function in R for every round of the optimisation process. Despite the more complicated prior used in later rounds in `case_study8.R` being designed to approximate yield curves, it may not be appropriate in a mixed fisheries context where the catch of one species is affected by the catch of another [@Originalpaper],[@MixME], [@Ulrich201238]. Leaving the GP more general avoids mis-specification, ensuring that the GP can be appropriately fitted to the points [@williams2006gaussian].

The process for the first half of the project adapted to our new situation is outlined again. We take a Bayesian History Matching (BHM) approach. Firstly, to get some initial data, we randomly sample our first set of $n-1$ points, where $n$ is the number of cores the system we are on has. Then, for each round we do the following:

-   We set up or update the Gaussian Processes (GPs) to model the $\textrm{min}(SSB)$ from 2030-2039 for each stock and the GP to model the total catch from 2030-2039

-   We can then use the $B_{lim}$ for each stock as a threshold so that we only consider the values of $F_{cod}$ and $F_{had}$ that have $SSB$ above $B_{lim}$ for all of the last ten years of the simulation (2030-2039)

-   We use the GP which is modelling the total catch to predict the value for the total catch at every point in the sample space, which will have some uncertainty

-   We use BHM to remove any points that are implausible (that have a low probability of being higher than the current best total catch)

-   We use the Knowledge Gradient (KG) acquisition function to select $n-1$ plausible points to sample in the next round, as per the discussion in [Deciding which acquisition function is best](https://github.com/dtj504-pixel/DissertationWork/blob/main/Deciding%20which%20acquisition%20function%20is%20best.qmd)

We repeat this process until there is only one plausible point left and then we will accept this as being the $F_{cod}$ and $F_{had}$ that maximise the catch whilst keeping the $SSB$ above $B_{lim}$.

## Calculating the risk

In contrast to the first half of the project, we now calculate the risk using the $SSB$. We have described it above briefly but will go into more detail here.

We should quickly note before our calculations that our new sample space is as below:

``` r
# Define Sample Space as discrete with 0.02 increments
dat <- data.frame(expand.grid(
  Fcod = seq(0.0, 0.6, by=0.02),
  Fhad = seq(0.0, 0.6, by=0.02)
))
```

This range ensures that we are able to model stock collapse for cod by modelling values above $F_{lim}$ and that we can model stock recovery for cod by including very low values [@ICES2019CodAdvice]. It also allows us to model unsafe fishing for haddock by modelling values above $F_{MSY}$ [@ICES2019HaddockAdvice]. Recalling that cod is the choke stock for both of our datasets, this sample space is appropriate [@MixMEwiki].

We also have a $B_{lim}$ for each stock, taken from ICES advice [@ICES2020HaddockBlim],[@ICES2019CodBlim]:

``` r
Blim_cod <- 107000
Blim_had <- 9227
```

Firstly, we extract the $\textrm{min}(SSB)$ for each stock from the result of the simulation we have run using the tracking object [@MixMEwiki],[@MixME]:

``` r
## // Extract the min ssb at the last ten years of the simulation for both stocks //
    # Picking up long term SSB values to see if they dip below Blim at any point
    ssb_cod_data <- c(res$tracking$cod$stk["SB.om", ac(2030:2039)])
    ssb_had_data <- c(res$tracking$had$stk["SB.om", ac(2030:2039)])

    # Getting minimum ssb during this time for each stock to model with GPs
    ssb_cod_min <- min(ssb_cod_data, na.rm = TRUE)
    ssb_had_min <- min(ssb_had_data, na.rm = TRUE)
```

We then put these results into a GP for each stock:

``` r
gp_cod_ssb <- km(~.^2,design=runs[,c("Fcod","Fhad")],estim.method="MLE",
                 response = ssb_cod_min,nugget=1e-12*var(ssb_cod_min)+1e-15,
                 covtype = "exp")
gp_had_ssb <- km(~.^2,design=runs[,c("Fcod","Fhad")],estim.method="MLE",
                 response = ssb_had_min,nugget=1e-12*var(ssb_had_min)+1e-15,
                 covtype = "exp")
```

These GPs have remained modelled by the `.^2` function, the same as in the GP for risk in the first half of the project, as this is quite general and avoids mis-specification [@williams2006gaussian]. They have also kept the same estimation method, nugget and kernel.

Next, we predict the $\textrm{min}(SSB)$ for every point in the sample space:

``` r
pred_ssb_cod <- predict(gp_cod_ssb, newdata = dat, type = "SK")
pred_ssb_had <- predict(gp_had_ssb, newdata = dat, type = "SK")
```

We then calculate $Risk = P(SSB < B_{lim})$ for each stock:

``` r
# Get the probability that sbb =< Blim for cod and haddock
pssb_cod <- pnorm(Blim_cod, pred_ssb_cod$mean, pred_ssb_cod$sd + 1e-12)
pssb_had <- pnorm(Blim_had, pred_ssb_had$mean, pred_ssb_had$sd + 1e-12)
```

Then, we set the KG of any unsafe points to be zero so that they will not be chosen as points to be sampled in the future:

``` r
# Loop over candidate points
  for (i in seq_len(m)) {
    # set to 0 for any unsafe points - any points with 
    # probability that ssb < Blim > 0.05 in the years 2030-2039 
    # i.e. the run is not precautionary in these years
    if (pssb_cod[i] > 0.05 || pssb_had[i] > 0.05) {
      kg[i] <- 0
      next
    }
```

We can say `ssb < Blim` here instead of `ssb =< Blim` because we are sampling from a continuous normal distribution and so they are equivalent.

By ensuring that $P(\textrm{min}(SSB) < B_{lim}) < 0.05$ we ensure that $P(SSB < B_{lim})<0.05$ for every year in the long term forecast (2030-2039). This guarantees that the maximum annual risk in this period remains below the 5% threshold, which is exactly what is required to meet the ICES precautionary standard due to their definition of $Prob3$ in [@ICES2019WKGMSE2]. This means that this risk calculation could be used in policy documents to set official catch limits in countries that have agreed to this standard [@ICES2019WKGMSE2].

## The basics of the MixME model

Going to write down everything and then remove areas Gustav said I don't need to worry about.

I have two slightly different pieces of code. Think the one I have put first uses less methods.

EXPLAIN why I start what the f_cod and f_had that I do

Optimising_ftarget_in_MixME_mult_points_parallel

-   Creating mixedfishery_MixME_om

-   Creating stk_oem

-   Management lag

-   Management type

-   obj_func

-   Check for advcie failure

-   Check for effort optimisation failure

-   tracking object

-   Check quota uptake

-   Check maximum overshoot

-   HCR parameters and setup

-   doOne

-   result object? mostly use tracking object from this

-   calculateQuotashare

-   stfMixME

-   Intial projection for the year 2020

-   FCB matrix

-   FLasher::fwdControl

-   FLasher::fwd

-   Cleaning up result object

Two_stocks_Optimising_ftarget_in_shortcut_model:

-   ICES_HCR function (including setting HCR parameters)

-   forecast_fun

-   Intial projection for the year 2020?

-   Set Forecast arguments on line 298 - think this is averaging parameters so don't need

-   Checking for effort optimisation or management procedure failures

-   Explaining the tracking object as this how I get SSB and catch

-   Explain how the data for this simulation is all the inputs from the previous one

-   Remove the simulation result at the end of loop and in doOne

# Further areas for Development

## Moving to a more complicated dataset

Mention the one in the MixME paper.

## Doing a proper risk calculation

We would need multiple iterations for this.

Precautionary is a requirement for many jurisdictions - I do now though, so would it be much of an improvement to use probabilities instead?