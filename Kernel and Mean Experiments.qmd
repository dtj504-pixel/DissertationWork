---
title: "Kernel and Mean Experiments Explained"
author: "Emma Bowen"
format: html 
editor: visual
bibliography: kernel_and_mean_references.bib
---

# Introduction

Along with my project supervisor, I decided to run some experiments determining whether changing the kernel and mean used in `case_study8.R` would have an effect on the value it determined as the optimal point. This was due to the fact that the exponential kernel, also called the Ornsteinâ€“Uhlenbeck correlation function, currently being used in the code (shown in @eq-originalkernel) was not as smooth as the Gaussian kernel (general form shown in @eq-GaussianKernel. This was pointed out and questioned by my supervisor the Gaussian kernel is more widely used [@williams2006gaussian].

$$
r_i(\phi,\phi',\delta_i) = \textrm{exp}\left(-\frac{|\phi_1-\phi_1'|}{\delta_{i,1}} - \frac{|\phi_2-\phi_2'|}{\delta_{i,2}}\right)
$$ {#eq-originalkernel}

$$
r(\phi) = \textrm{exp}\left(-\frac{r^2}{2l^2}\right)
$$ {#eq-GaussianKernel}

The mean was also identified as a possible issue. By assuming a very specific mean (shown below @eq-gpmean) for the Gaussian process we are assuming that we know a lot about the objective function $f$ (which is what we are modeling with our Gaussian Process) [@williams2006gaussian]. Thus, it was suggested that I change the code to a zero mean which is commonly done when we do not want to make many assumptions about our objective function [@williams2006gaussian].

$$
\begin{split}
m_1(\phi) = \beta_{1,0} + \beta_{1,1}(ln(\phi_1 +0.1))+\beta_{1,2}(ln(\phi_1+0.1))^2+ \\
\beta_{1,3}(ln(\phi_1+0.1))^3+\beta_{1,4}(\phi_2ln(\phi_1+0.1))+\beta_{1,5}\phi_2
\end{split}
$$ {#eq-gpmean}

where $$\phi_1 = \frac{F_{target}-0.1}{0.4} \quad \textrm{and} \quad \phi_2 = \frac{B_{trigger}-110000}{90000}$$

## Experimenting with the kernel

The assumption of the Gaussian kernel of infinite differentiability can be too strong for real-world processes as it is very smooth [@williams2006gaussian]. The GP will create a very smooth surface due to the very smooth assumption in the Gaussian kernel \[@williams2006gaussian\]. If the new point does not fit the very smooth assumption from the Gaussian kernel, then the GP is unsure how to proceed and so sets the variance very high as if it knew nothing, sometimes called a variance explosion [@williams2006gaussian]. This results in many points that had been deemed implausible returning and this is likely to continue unless different sampling points are chosen [@williams2006gaussian]. It is instead recommended that we use a kernel from the Matern class of kernels, which includes the exponential kernel [@williams2006gaussian].

My experiments appear to back up this theory - RUN THEM AGAIN AND GET HEATMAPS TO SHOW,ETC.

## Experimenting with the mean

### References