<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Emma Bowen">

<title>Explanation of Mathematical Methods Behind the First Half of the Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/clipboard/clipboard.min.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/popper.min.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/anchor.min.js"></script>
<link href="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/bootstrap/bootstrap-55aa396ea3e988129bcec43acf404917.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglWebGL-binding-1.3.31/rglWebGL.js"></script>
<link href="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/rgl.css" rel="stylesheet">
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/rglClass.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/utils.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/buffer.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/subscenes.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/shaders.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/shadersrc.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/textures.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/projection.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/mouse.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/init.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/pieces.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/draw.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/controls.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/selection.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/rglTimer.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/pretty.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/axes.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/rglwidgetClass-1.3.31/animation.src.js"></script>
<script src="No Code Explanation of Mathematical Methods From First Half of the Project_files/libs/CanvasMatrix4-1.3.31/CanvasMatrix.src.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Explanation of Mathematical Methods Behind the First Half of the Project</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Emma Bowen </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>What mathematical methods will I explain in this document?</p>
<ul>
<li><p>Gaussian processes</p></li>
<li><p>Bayesian History Matching</p></li>
<li><p>Kriging</p></li>
<li><p>Expected Improvement</p></li>
<li><p>Augmented Expected Improvement</p></li>
<li><p>Knowledge Gradient</p></li>
<li><p>K-Means Clustering</p></li>
</ul>
</section>
<section id="context" class="level1">
<h1>Context</h1>
<p>We are building on the paper “Using history matching to speed up management strategy evaluation grid searches”. This paper is looking to find the Harvest Control Rule parametrised by <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that maximises the catch whilst keeping the risk below 0.05 for a single-stock fishery. The paper does not consider fleet dynamics. We have an objective function in the paper which combines the risk and the catch and so this is the function we want to maximise to get our maximal catch with the constraint of keeping the risk below 0.05.</p>
<p>To do this, the paper takes a Bayesian History Matching (BHM) approach. Firstly, we sample our first eight points which are spaced evenly throughout the sample space to get some initial data. Then, for each round we do the following:</p>
<ul>
<li><p>We set up or update the Gaussian Process (GP) to model the risk and the GP to model the catch</p></li>
<li><p>We can then use the risk as a threshold so that we only consider the values of <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that have risk below 0.05</p></li>
<li><p>We use the GP which is modelling the catch to get the value for the catch at every point in the sample space, which will have some uncertainty</p></li>
<li><p>We use BHM to remove any points that are implausible (that have a low probability of being higher than the current best catch)</p></li>
<li><p>We select 8 plausible points to sample in the next round</p></li>
</ul>
<p>We repeat this process until there is only one plausible point left and then we will accept this as being the <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that maximise the catch whilst keeping the risk below 0.05.</p>
<p>Now, we will look at the mathematics behind the methods above and also at the acquisition functions of Expected Improvement, Augmented Expected Improvement and Knowledge Gradient which I added to the original code.</p>
</section>
<section id="general-theory" class="level1">
<h1>General theory</h1>
<section id="gaussian-processes" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-processes">Gaussian Processes</h2>
<p>A Gaussian Process <span class="math inline">\(F\)</span> has a mean function <span class="math inline">\(\mu_0\)</span> and a covariance function <span class="math inline">\(\operatorname{cov}_0(x_i,x_j)\)</span>. We can then evaluate the covariance function <span class="math inline">\(\operatorname{cov}_0(x_i,x_j)\)</span> for every pair <span class="math inline">\(x_i,x_j\)</span> where <span class="math inline">\(i,j\in\{1, ..., n\}\)</span> to find the covariance matrix <span class="math inline">\(\Sigma_{1:n}\)</span> . Then, <span class="math inline">\(F\)</span> is a probability distribution over our objective function <span class="math inline">\(f\)</span> with the property that, for any given collection of points <span class="math inline">\({x_1,...x_n}\)</span>, the marginal probability distribution on <span class="math inline">\(F(x_{1:n}) = (F(x_1),...,F(x_n))\)</span> is given by <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span id="eq-MultivariateNormalDist"><span class="math display">\[
F(x_{1:n}) \sim N((\mu_0(x_{1:n}),\Sigma_{1:n})
\tag{1}\]</span></span></p>
<p>where</p>
<p><span class="math display">\[
\mu_0(x_{1:n}) = (\mu_0(x_1), . . ., \mu_0(x_n))
\]</span></p>
<p>We choose a covariance function such that inputs that have nearby points that have been evaluated have a more certain output than points that are further away from the points that have been evaluated <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This is equivalent to saying that if for some <span class="math inline">\(x,x',x''\)</span> in the design space we have <span class="math inline">\(\|x - x'\| &lt; \|x -x''\|\)</span> for some norm <span class="math inline">\(\| \cdot \|\)</span>, then <span class="math inline">\(\operatorname{cov}_0(x, x') &gt; \operatorname{cov}_0(x, x'')\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>We use a GP to emulate the objective function because it is much cheaper to evaluate than our objective function. We can calculate <span class="math inline">\(F(x)\)</span> for any <span class="math inline">\(x\)</span> in the design space as our estimate of <span class="math inline">\(f(x)\)</span> based on our current beliefs. This is true even for the evaluated points <span class="math inline">\(x_1,...,x_m\)</span> as the emulator is fitted to these points <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
</section>
<section id="maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<p>When using GPs to emulate our objective function, we need to be able to estimate the coefficients of the objective function using the data we gain from evaluating our points <span class="math inline">\(x_1,...,x_n\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>We can do this as follows. Firstly, we let the vector <span class="math inline">\(\eta\)</span> represent the hyperparameters that give us <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\operatorname{cov}_0\)</span>. Then, given the observations <span class="math inline">\(f(x_{1:n}) = (f(x_1),...,f(x_n))\)</span>, we calculate the likelihood of these observations under the prior given <span class="math inline">\(\eta\)</span> which is denoted as <span class="math inline">\(p(f(x_{1:n})|\eta)\)</span> which is modelled by <a href="#eq-MultivariateNormalDist" class="quarto-xref">Equation&nbsp;1</a> . Lastly, we set <span class="math inline">\(\hat{\eta}\)</span> to the value that maximizes this likelihood <span class="citation" data-cites="BayesianOptimisationTutorial">Frazier (<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">2018</a>)</span>]:</p>
<p><span class="math display">\[
\hat{\eta} = argmax_\eta p(f(x_{1:n})|\eta)
\]</span></p>
</section>
<section id="kriging" class="level2">
<h2 class="anchored" data-anchor-id="kriging">Kriging</h2>
<p>Kriging is a Bayesian statistical method for modelling functions <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. Again, let <span class="math inline">\(f\)</span> be the objective function and we focus on the design space <span class="math inline">\(X := \{x_1,...,x_n\}\)</span>. Now, if we have evaluated <span class="math inline">\(n\)</span> points such that we have <span class="math inline">\(f(x_{1:n})\)</span> and want to evaluate <span class="math inline">\(x_{n+1}\)</span> we let <span class="math inline">\(k = n+1\)</span> in <a href="#eq-MultivariateNormalDist" class="quarto-xref">Equation&nbsp;1</a> . Then, we can compute the conditional distribution of <span class="math inline">\(F(x_{n+1})\)</span> given <span class="math inline">\(f(x_{1:n})\)</span> using Bayes’ rule:</p>
<p><span id="eq-posteriordistributiongivensamples"><span class="math display">\[
F(x_{n+1})|f(x_{1:n}) \sim N(\mu_n(x_{n+1}), \sigma^2_n(x_{n+1}))
\tag{2}\]</span></span></p>
<p>where:</p>
<p><span class="math display">\[
mu_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}(f(x_{1:n}) - \mu_0(x_{1:n}))  + \mu_0(x_{n+1})
\]</span></p>
<p><span class="math display">\[
\sigma^2_n(x_{n+1}) = \operatorname{cov}_0(x_{n+1},x_{n+1}) - \operatorname{cov}_0(x_{n+1}, x_{1:n})(\Sigma_{1:n})^{-1}\operatorname{cov}_0(x_{1:n},x_{n+1})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\operatorname{cov}_0(x_{n+1}, x_{1:n}) = (\operatorname{cov}_0(x_{n+1}, x_1), ... , \operatorname{cov}_0(x_{n+1}, x_n))
\]</span></p>
<p>This conditional distribution <span class="math inline">\(F(x_{n+1})|f(x_{1:n})\)</span> is called the posterior probability distribution for <span class="math inline">\(x_{n+1}\)</span>. We can calculate this distribution for every point in the design space <span class="math inline">\(X\)</span>. This results in a new GP <span class="math inline">\(F_n\)</span> with a mean vector and covariance kernel that depend on the location of the unevaluated points, the locations of the evaluated points <span class="math inline">\(x_{1:n}\)</span>, and their values <span class="math inline">\(f(x_{1:n})\)</span><span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. So, we can update our GP every round based on the new points we have evaluated.</p>
</section>
<section id="bayesian-history-matching" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-history-matching">Bayesian History Matching</h2>
<p>Let <span class="math inline">\(x\)</span> be a point in the sample space. We begin with some uncertainty about our objective function <span class="math inline">\(f(x)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. However, we can make probabilistic statements such as:</p>
<p><span class="math display">\[\begin{equation}
    P(f(x)&gt;a)= \int_{a}^{\infty}P(f(x))df(x)
\end{equation}\]</span></p>
<p>Once we evaluate another point <span class="math inline">\(x'\)</span> where <span class="math inline">\(x' \neq x\)</span>, we are able to use Bayes’ Theorem improve our integral to:</p>
<p><span class="math display">\[\begin{equation}
    P(f(x)&gt;a|f(x'))= \int_{a}^{\infty}P(f(x)|f(x'))df(x)
\end{equation}\]</span></p>
<p>We now let <span class="math inline">\(a=max\{f(x_1),...,f(x_n)\}\)</span> where <span class="math inline">\(n\)</span> is the number of points we have evaluated so far. For the first round, <span class="math inline">\(n=8\)</span> but as the rounds increase we make sure to include all previous points of the objective function that have been evaluated.<span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span> We remove the point <span class="math inline">\(x\)</span> if:</p>
<p><span id="eq-removeimplausiblepoint"><span class="math display">\[
P(f(x)&gt;a|f(x_{1:n}) = \int_{a}^{\infty}P(f(x)|f(x_{1:n})df(x) &lt;\varepsilon
\tag{3}\]</span></span></p>
<p>for a small <span class="math inline">\(\varepsilon &gt; 0\)</span> until no plausible points remain. Then, the optimum will be <span class="math inline">\(x^*\)</span> such that:</p>
<p><span class="math display">\[
f(x^*) = max\{f(x_{1:n})\}
\]</span></p>
<p>as our index <span class="math inline">\(n\)</span> counts the number of points we have evaluated throughout the whole simulation.</p>
</section>
<section id="expected-improvement" class="level2">
<h2 class="anchored" data-anchor-id="expected-improvement">Expected Improvement</h2>
<p>The first type of acquisition function we will look at is Expected Improvement (EI). Suppose we have sampled the points <span class="math inline">\(x_1, ... ,x_n\)</span> and observe the values <span class="math inline">\(f(x_{1:n})\)</span>. Then, if we were to return a solution at this point, bearing in mind we observe the objective function <span class="math inline">\(f\)</span> without noise and we can only return points we have already evaluated, we would return <span class="math inline">\(f^*_n = max\{f(x_1),...,f(x_n)\}\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. Imagine we then consider evaluating another point <span class="math inline">\(x_{n+1}\)</span> to get <span class="math inline">\(f(x_{n+1})\)</span>. We can then define the Expected Improvement as:</p>
<p><span class="math display">\[\begin{equation}
    EI_n(x_{n+1}) := E[F(x_{n+1})|f(x_{1:n})-f^*_n]^+
\end{equation}\]</span></p>
<p>where <span class="math inline">\([F(x_{n+1})-f^*_n]^+\)</span> is the positive part of <span class="math inline">\([F(x_{n+1})-f^*_n]\)</span>. This acquisition function is relatively easy to optimise and many different methods have been developed for doing this <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>There is another expression for <span class="math inline">\(EI_n(x_{n+1})\)</span>:</p>
<p><span id="eq-EIincode"><span class="math display">\[
EI_n(x_{n+1}) =  [(\mu_n(x_{n+1})-f_n^*\cdot\Phi(Z))+(\sigma_n(x_{n+1})\cdot\phi(Z))]^+
\tag{4}\]</span></span></p>
<p>where again the notation <span class="math inline">\([\cdot]^+\)</span> means the positive part and where:</p>
<p><span class="math display">\[
Z = \frac{\mu_n(x_{n+1})-f_n^*}{\sigma_n(x_{n+1})}
\]</span></p>
<p><a href="#eq-EIincode" class="quarto-xref">Equation&nbsp;4</a> can be gained from <strong>?@eq-priordistribution</strong> by setting <span class="math inline">\(k = n+1\)</span> and then studying the distribution of <span class="math inline">\(F(x_{n+1})-f^*_n\)</span>. However, we can also consider it as a version of Equation (15) from <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span> where we first flip the signs as we are focused on the maximisation case and then set <span class="math inline">\(f_{min} = f^*_n\)</span>, <span class="math inline">\(\hat{y} = \mu_n(x)\)</span> and <span class="math inline">\(s = \sigma_n(x)\)</span>.</p>
</section>
<section id="augmented-expected-improvement" class="level2">
<h2 class="anchored" data-anchor-id="augmented-expected-improvement">Augmented Expected Improvement</h2>
<p>This was included to help make the method perform better for noisy functions which will make it more generally applicable <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>. To deal with these noisy observations, a change was proposed to standard EI function as detailed below. This change seems mostly to have been justified by empirical performance <span class="citation" data-cites="letham2018constrainedbayesianoptimizationnoisy">(<a href="#ref-letham2018constrainedbayesianoptimizationnoisy" role="doc-biblioref">Letham et al. 2018</a>)</span>.</p>
<p>By adjusting Equation (12) found in <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span> to our own notation, we get that:</p>
<p><span class="math display">\[\begin{equation}
    AEI_n(x_{n+1})=E_n[F(x_{n+1})|f(x_{1:n})-f^*_{eb}]^+\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma_{obs}\)</span> is the standard deviation of the noise variable set by the user and <span class="math inline">\(\sigma_n(x)\)</span> is the standard deviation of GP <span class="math inline">\(F\)</span> at the <span class="math inline">\(n^{th}\)</span> iteration, as used beforehand. We have also changed <span class="math inline">\(f^*_n\)</span> to <span class="math inline">\(f^*_{eb}\)</span> which is the highest predicted mean at any sampled point so far so that we take into account that the uncertainty in our observations could cause a large spike <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
</section>
<section id="knowledge-gradient" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-gradient">Knowledge Gradient</h2>
<p>We remove the assumption of EI that we have to return a pre-evaluated point as our best point <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. This allows us to do some different computations to the ones in EI. We also now start by saying that the solution we would choose if we have to stop sampling after n points would be the point in the design space with the largest <span class="math inline">\(\mu_n(\cdot)\)</span> value, where <span class="math inline">\(\mu_n(\cdot)\)</span> is the mean vector of the posterior probability distribution after <span class="math inline">\(n\)</span> iterations. We call this maximum <span class="math inline">\({x_n^*}\)</span> and then can say that <span class="math inline">\(F(x_n^*)\)</span> is random under the posterior distribution and has the mean vector after sampling <span class="math inline">\(f(x_{1:n})\)</span> of:</p>
<p><span class="math display">\[\begin{equation}
    \mu_n^* := \mu_n(x_n^*) = max_{x}\mu_n(x)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x\)</span> is any point in the sample space <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>Then, we imagine that we are now allowed to sample a new point <span class="math inline">\(x_{n+1}\)</span>. We get a new posterior distribution at the point <span class="math inline">\(x\)</span> which we can calculate using <a href="#eq-posteriordistributiongivensamples" class="quarto-xref">Equation&nbsp;2</a> by replacing <span class="math inline">\(x_{n+1}\)</span> with <span class="math inline">\(x\)</span> and <span class="math inline">\(x_{1:n}\)</span> with <span class="math inline">\(x_{1:n+1}\)</span> to include our new observation. This will have the posterior mean function <span class="math inline">\(\mu_{n+1}(\cdot)\)</span> and the conditional expected value for <span class="math inline">\(F(x_n^*)\)</span> changes to be:</p>
<p><span class="math display">\[\begin{equation}
    \mu_{n+1}^* := max_x\mu_{n+1}(x)
\end{equation}\]</span></p>
<p>So, we can see that the increase in the conditional expected value of <span class="math inline">\(F(x_n^*)\)</span> by sampling the new point <span class="math inline">\(x_{n+1}\)</span> is <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span class="math display">\[\begin{equation}
    \mu_{n+1}^* - \mu_n^*
\end{equation}\]</span></p>
<p>While this quantity is unknown before we sample <span class="math inline">\(x_{n+1}\)</span> we can calculate it’s expected value given our observations <span class="math inline">\(x_1,...,x_n\)</span>. The Knowledge Gradient for sampling at a new point <span class="math inline">\(x\)</span> in the design space is defined as <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>:</p>
<p><span id="eq-KnowledgeGradient"><span class="math display">\[
KG_n(x) := E_n[\mu_{n+1}^* - \mu_n^*|x_{n+1} = x]
\tag{5}\]</span></span></p>
<p>where again <span class="math inline">\(E_n\)</span> indicates the expectation taken under the posterior distribution at the <span class="math inline">\(n^{th}\)</span> iteration. We would sample the point <span class="math inline">\(x\)</span> with the largest <span class="math inline">\(KG_n(x)\)</span> as our next point <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>The easiest way to calculate the KG is via simulation. This can be done by simulating one possible value for <span class="math inline">\(f(x_{n+1})\)</span>. Then, we calculate <span class="math inline">\(\mu_{n+1}^*\)</span> and subtract <span class="math inline">\(\mu_n^*\)</span>. We iterate this process many times so that we can find the average of <span class="math inline">\(\mu_{n+1}^* - \mu_n^*\)</span> and this allows us to estimate <span class="math inline">\(KG_n(x)\)</span> <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. This process, or calculating <a href="#eq-KnowledgeGradient" class="quarto-xref">Equation&nbsp;5</a> directly from the properties of the normal distribution, both work well in discrete, low dimensional problems which is the situation we are in for the first half of the project <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>.</p>
<p>Alternatively, we can calculate <span class="math inline">\(\mu_{n+1}\)</span> using the formula below <span class="citation" data-cites="ungredda2022efficientcomputationknowledgegradient">(<a href="#ref-ungredda2022efficientcomputationknowledgegradient" role="doc-biblioref">Ungredda, Pearce, and Branke 2022</a>)</span>:</p>
<p><span id="eq-updatemuKG"><span class="math display">\[
\mu_{n+1}(x) = \mu_n(x) + \frac{\operatorname{cov}_n(x_{n+1},x)}{\operatorname{var}_n(x_{n+1}) + \sigma_{\mathrm{obs}}^2}(F(x_{n+1})-\mu_n(x_{n+1}))
\tag{6}\]</span></span></p>
<p>where <span class="math inline">\(\sigma_{obs}\)</span> is a noise variable which can be determined by the user <span class="citation" data-cites="ungredda2022efficientcomputationknowledgegradient">(<a href="#ref-ungredda2022efficientcomputationknowledgegradient" role="doc-biblioref">Ungredda, Pearce, and Branke 2022</a>)</span>. From the GP for catch, we get <span class="math inline">\(\operatorname{cov}_n(x_{n+1},x)\)</span> and the standard deviation <span class="math inline">\(\sigma_{\mathrm{obs}}^2\)</span>.</p>
</section>
<section id="kmeans-process-for-selecting-multiple-points" class="level2">
<h2 class="anchored" data-anchor-id="kmeans-process-for-selecting-multiple-points">Kmeans process for selecting multiple points</h2>
<p>HIGHLIGHT THAT COMBINING THIS WITH AN ACQUISITION FUCNTION WAS AN IDEA I HAD BEFRE READINGA NY LITERATURE ON IT ( AS I THINK THERE IS SOME).</p>
<p>We have now been able to determine which points are possible based on the probability that their catch is higher than the current maximum catch (using Bayesian History Matching) and the probability that their risk is less than 0.05. These points will be called the Possible Space, <span class="math inline">\(PS\)</span>. We have then assigned a value to each point in <span class="math inline">\(PS\)</span> using one of the acquistion functions above. Now, we want to decide which 8 points are best to evaluate next.</p>
<p>For sampling only one point next, this would be very simple as you would take the point with the highest value assigned by the acquisition function <span class="citation" data-cites="BayesianOptimisationTutorial">(<a href="#ref-BayesianOptimisationTutorial" role="doc-biblioref">Frazier 2018</a>)</span>. However, we want to sample 8 points next so that we continue the pattern set up in the original paper and we want a good trade-off between exploration and exploitation <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>,<span class="citation" data-cites="batchspreadinoutjustification">(<a href="#ref-batchspreadinoutjustification" role="doc-biblioref"><strong>batchspreadinoutjustification?</strong></a>)</span>.</p>
<p>So, we use the <code>kmeans</code> function which is part of the stats package in R (which is automatically loaded into an R session). This function uses the algorithm from Hartigan and Wong, 1979 by default<span class="citation" data-cites="AKMeansClusteringAlgorithm">(<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">Hartigan and Wong 2018</a>)</span>,<span class="citation" data-cites="kmeansdocumentation">(<a href="#ref-kmeansdocumentation" role="doc-biblioref">RDocumentation 2025</a>)</span>. The k-means clustering method is the most commonly used due to its simplicity compared to other clustering algorithms<span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>.)</span></p>
<p>This algoritm creates <span class="math inline">\(k\)</span> clusters (groups of points) such that the points within each cluster have the sum of squares to the centre of their cluster smaller than it would be to the centre of any other cluster <span class="citation" data-cites="kmeansdocumentation">(<a href="#ref-kmeansdocumentation" role="doc-biblioref">RDocumentation 2025</a>)</span>. It starts by defining <span class="math inline">\(k\)</span> centroids which should be placed as much as possible far away from each other <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. Then, we take each point in the space and associate it to the nearest centroid. We stop when every point has been assigned to a centroid <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. At this point, we re-calculate <span class="math inline">\(k\)</span> new centroids as the centers of the clusters created by the previous step. This may result in some points changing clusters <span class="citation" data-cites="AKMeansClusteringAlgorithm">(<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">Hartigan and Wong 2018</a>)</span>. We repeat this process until no points change clusters <span class="citation" data-cites="AKMeansClusteringAlgorithm">(<a href="#ref-AKMeansClusteringAlgorithm" role="doc-biblioref">Hartigan and Wong 2018</a>)</span>,<span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>.</p>
<p>However, before the algorithm can start, we must specify how many clusters we want <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. This can be difficult in many cases <span class="citation" data-cites="kodinariya2013review">(<a href="#ref-kodinariya2013review" role="doc-biblioref">Kodinariya, Makwana, et al. 2013</a>)</span>. In our case, it is relatively simple as we know how many points we want to sample next and so we set this to be the number of clusters. We then run the algorithm on <span class="math inline">\(PS\)</span> to form the clusters. Then, we pick the point with the highest value of the acquisition function from each cluster to sample in our next round. This allows us to search for viable points by looking in the Possible Space but also to keep the points we are going to sample spread out so that we can balance exploitation and exploration more effectively <span class="citation" data-cites="batchspreadingoutjustification">(<a href="#ref-batchspreadingoutjustification" role="doc-biblioref">Azimi, Fern, and Fern 2010</a>)</span>.</p>
</section>
</section>
<section id="application-of-theory-in-my-project" class="level1">
<h1>Application of theory in my project</h1>
<section id="set-up-the-gaussian-processes" class="level2">
<h2 class="anchored" data-anchor-id="set-up-the-gaussian-processes">Set up the Gaussian Processes</h2>
<p>WHAT OBJECTIVE FUNCTION ARE WE MAXIMISING AND WHY DO WE USE TWO SEPARATE GPs?</p>
<p>The risk GP models <span class="math inline">\(ln(risk)\)</span> and the catch GP models <span class="math inline">\(ln(catch)\)</span> . To maintain the notation from the Spence 2025 paper, we use <span class="math inline">\(m_1\)</span> for the mean function of the catch GP and <span class="math inline">\(m_2\)</span> for the mean function of the risk GP <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>:</p>
<p><span class="math display">\[
\begin{split}
m_1 (\phi) = \beta_{1,0} + \beta_{1,1} (\operatorname{ln}(\phi_1 + 0.1))+\beta_{1,2}(\operatorname{ln}(\phi_1 + 0.1))^2 + \\
\beta_{1,3}(\operatorname{ln}(\phi_1 + 0.1))^3+ \beta_{1,4}(\phi_2\operatorname{ln}(\phi_1 + 0.1)) + \beta_{1,5}\phi_2
\end{split}
\]</span></p>
<p><span class="math display">\[
m_2(\phi) = \beta_{2,0} + \beta_{2,1}\phi_1 + \beta_{2,2}\phi_2 + \beta_{2,3}\phi_1\phi_2
\]</span></p>
<p>where all of the above <span class="math inline">\(\beta_{s,t}\)</span> for <span class="math inline">\(s \in \{1,2\}\)</span> and <span class="math inline">\(t \in \{1,2,3,4,5\}\)</span> are coefficients to be found through maximum likelihood estimation and:</p>
<p><span class="math display">\[
\phi_1 = \frac{F_{target}-0.1}{0.4} \quad \textrm{and} \quad \phi_2 = \frac{B_{trigger}-110000}{90000}
\]</span></p>
<p>where we rescale for numerical stability in the GP <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
<p>Our covariance function <span class="math inline">\(c\)</span> for both GPs is the variance <span class="math inline">\(\sigma_i^2\)</span> (which is acting as a scalar) times the Ornstein-Uhlenbeck correlation function <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>:</p>
<p><span class="math display">\[
r_i(\phi,\phi',\delta_i) = \operatorname{exp} \left(-\frac{|\phi_1 - \phi'_1|}{\delta_{i,1}} - \frac{|\phi_2 - \phi_2'|}{\delta_{i,2}}\right)
\]</span></p>
<p>where <span class="math inline">\({\delta_{i,1}}\)</span> and <span class="math inline">\({\delta_{i,2}}\)</span> are the length scales for each of the <span class="math inline">\(\phi_1\)</span> and <span class="math inline">\(\phi_2\)</span> terms respectively <span class="citation" data-cites="williams2006gaussian">(<a href="#ref-williams2006gaussian" role="doc-biblioref"><strong>williams2006gaussian?</strong></a>)</span>.</p>
<p>We need to sample our first round of eight points before setting up the GPs so that we have enough data to estimate all of the coefficients in our GPs <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>. Note that until we have sampled sixteen points, we set the prior of the catch GP to be the same as the risk GP, <span class="math inline">\(m_2(\phi)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. This is because we need to estimate the coefficients for the mean function for the catch, risk and the length scales for the covariance function <span class="math inline">\(c\)</span> <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>,<span class="citation" data-cites="Roustant2012DiceKrigingpaper">(<a href="#ref-Roustant2012DiceKrigingpaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012</a>)</span>. For mathematical stability, these functions cannot have more than eight coefficients between them in our first round as we are only sampling eight points per round due to computation limitations encountered at the time of the Spence 2025 paper <span class="citation" data-cites="EfficientGlobalOptimizationofExpensiveBlackBoxFunctions">(<a href="#ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>,<span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. However, after our first round we reset the mean function for the catch GP to <span class="math inline">\(m_1(\phi)\)</span> <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
<p>We can set up Gaussian Processes (GPs) as described above in R using the DiceKriging package and use maximum likelihood estimate to get the hyperparameters of <span class="math inline">\(m_1(\phi),m_2(\phi)\)</span> and <span class="math inline">\(c\)</span> for our GPs <span class="citation" data-cites="DiceKrigingDocumentation">(<a href="#ref-DiceKrigingDocumentation" role="doc-biblioref">Roustant 2025</a>)</span>,<span class="citation" data-cites="Roustant2012DiceKrigingpaper">(<a href="#ref-Roustant2012DiceKrigingpaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012</a>)</span>. Then, we use them to predict the <span class="math inline">\(ln(catch)\)</span> and <span class="math inline">\(ln(risk)\)</span> at every point in the design space. We can then exponentiate these results where needed. We are building our GPs with <span class="math inline">\(ln\)</span> of the values we want because this helps us generate better predictions <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
<p>We have been able to visualise the GPs after the first round in 3D:</p>
<p>The middle plane is the mean of the GP, whereas the bottom and top planes represent the lower and upper bounds of the 95% confidence interval respectively. The planes meet at the evaluated points, which are the red dots on the diagram. The scales are odd due to the re-scaling of <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> and fitting our GPs to log of the values we want throughout the code which helps to keep the GP stable <span class="citation" data-cites="GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels">(<a href="#ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" role="doc-biblioref">Huang et al. 2006</a>)</span>.</p>
<p>We can then also do this for the GP for catch:</p>
</section>
<section id="excluding-implausible-points" class="level2">
<h2 class="anchored" data-anchor-id="excluding-implausible-points">Excluding implausible points</h2>
<p>Firstly, we enforce the precautionary threshold of <span class="math inline">\(P(log(risk)\le0.05)\)</span> by calculating:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>prisk <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="fu">log</span>(<span class="fl">0.05</span>),pred_risk_g<span class="sc">$</span>mean,pred_risk_g<span class="sc">$</span>sd<span class="fl">+1e-12</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where we include <code>+1e-12</code> in the above to prevent the variance becoming negative due to imprecision when calculating <code>pred_risk_g$sd</code>. This would encounter an error from the <code>pnorm</code> function which would stop the code <span class="citation" data-cites="DiceKrigingDocumentation">(<a href="#ref-DiceKrigingDocumentation" role="doc-biblioref">Roustant 2025</a>)</span>,<span class="citation" data-cites="Roustant2012DiceKrigingpaper">(<a href="#ref-Roustant2012DiceKrigingpaper" role="doc-biblioref">Roustant, Ginsbourger, and Deville 2012</a>)</span>.</p>
<p>Then, Bayesian history matching speeds up the process by removing any points that are implausible according to <a href="#eq-removeimplausiblepoint" class="quarto-xref">Equation&nbsp;3</a>. In our case, <span class="math inline">\(x^*\)</span> is the <span class="math inline">\(F_{target}\)</span> and <span class="math inline">\(B_{trigger}\)</span> that will give the highest catch whilst following the precautionary principle <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>.</p>
</section>
<section id="deciding-on-next-point-to-sample" class="level2">
<h2 class="anchored" data-anchor-id="deciding-on-next-point-to-sample">Deciding on next point to sample</h2>
<p>We have three different acquisition functions we have investigated using here..</p>
<section id="expected-improvement-1" class="level3">
<h3 class="anchored" data-anchor-id="expected-improvement-1">Expected Improvement</h3>
<p>We use the <a href="#eq-EIincode" class="quarto-xref">Equation&nbsp;4</a> expression. This method of EI can be coded up as the function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>expected_improvement <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma, y_best, <span class="at">xi =</span> <span class="fl">0.05</span>, <span class="at">task =</span> <span class="st">"max"</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                                 pred_risk, <span class="at">eps =</span> <span class="fl">1e-4</span>) </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Setting up EI vector</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  ei <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(mu))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Determining points with P(risk&lt;=0.05)&gt;0.00001</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  safe_points <span class="ot">&lt;-</span> pred_risk <span class="sc">&gt;=</span> eps</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Only calculate EI if there are safe points and the task is a valid option</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">any</span>(safe_points)) {</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (task <span class="sc">==</span> <span class="st">"min"</span>) </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        imp <span class="ot">&lt;-</span> y_best <span class="sc">-</span> mu[safe_points] <span class="sc">-</span> xi</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (task <span class="sc">==</span> <span class="st">"max"</span>) </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        imp <span class="ot">&lt;-</span> mu[safe_points] <span class="sc">-</span> y_best <span class="sc">-</span> xi </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="fu">stop</span>(<span class="st">'task must be "min" or "max"'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> imp <span class="sc">/</span> sigma[safe_points]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Only calculate EI for safe points</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    ei[safe_points] <span class="ot">&lt;-</span> imp <span class="sc">*</span> <span class="fu">pnorm</span>(Z) <span class="sc">+</span> sigma[safe_points] <span class="sc">*</span> <span class="fu">dnorm</span>(Z)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    ei[safe_points][sigma[safe_points] <span class="sc">==</span> <span class="fl">0.0</span>] <span class="ot">&lt;-</span> <span class="fl">0.0</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(ei)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where <code>mu</code> is the mean of the GP for log(catch) and <code>sigma</code> is the standard deviation of the GP for log(catch). Then we also have <code>y_best</code> which is the current best log(catch), <code>xi</code> which is a parameter balancing exploration and exploitation. We set <code>pred_risk = prisk</code> and <code>eps = 1e-4</code> and these are the same as before.</p>
</section>
<section id="augmented-expected-improvement-1" class="level3">
<h3 class="anchored" data-anchor-id="augmented-expected-improvement-1">Augmented Expected Improvement</h3>
<p>As our situation has no noise, in our code we are still using <span class="math inline">\(f^*_n\)</span><span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Thus, we code up the equation:</p>
<p><span class="math display">\[\begin{equation}
    AEI_n(x_{n+1})=EI_n(x_{n+1})\left(1- \frac{\sigma_{obs}}{\sqrt{\sigma_n^2(x_{n+1})+\sigma_{obs}^2}}\right)
\end{equation}\]</span></p>
<p>in the code below:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>augmented_expected_improvement <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma, y_best, <span class="at">xi =</span> <span class="fl">0.05</span>, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                                           <span class="at">task =</span> <span class="st">"max"</span>, pred_risk, <span class="at">eps =</span> <span class="fl">1e-4</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                                           <span class="at">noise_var =</span> <span class="dv">0</span>) </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  ei <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(mu))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  safe_points <span class="ot">&lt;-</span> pred_risk <span class="sc">&gt;=</span> eps</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Only calculate AEI for safe points</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">any</span>(safe_points))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (task <span class="sc">==</span> <span class="st">"min"</span>) </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            imp <span class="ot">&lt;-</span> y_best <span class="sc">-</span> mu[safe_points] <span class="sc">-</span> xi</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (task <span class="sc">==</span> <span class="st">"max"</span>) </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            imp <span class="ot">&lt;-</span> mu[safe_points] <span class="sc">-</span> y_best <span class="sc">-</span> xi </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            <span class="fu">stop</span>(<span class="st">'task must be "min" or "max"'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        Z <span class="ot">&lt;-</span> imp <span class="sc">/</span> sigma[safe_points]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        ei[safe_points] <span class="ot">&lt;-</span> imp <span class="sc">*</span> <span class="fu">pnorm</span>(Z) <span class="sc">+</span> sigma[safe_points] <span class="sc">*</span> <span class="fu">dnorm</span>(Z)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ei[safe_points][sigma[safe_points] <span class="sc">==</span> <span class="fl">0.0</span>] <span class="ot">&lt;-</span> <span class="fl">0.0</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Augmentation factor to handle noise</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># When noise_var = 0 it reduces to standard EI</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        augmentation_factor <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sqrt</span>(noise_var <span class="sc">/</span> (noise_var <span class="sc">+</span> sigma<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        aei <span class="ot">&lt;-</span> ei <span class="sc">*</span> augmentation_factor</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Giving points with prob(risk &lt;= 0.05) &lt; 0.0001 an expected </span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># improvement of zero so we avoid them</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  aei <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(pred_risk <span class="sc">&lt;</span> eps, <span class="dv">0</span>, aei)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(aei)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where <code>noise_var</code> corresponds to <span class="math inline">\(\sigma_{obs}^2\)</span> and <code>sigma</code> corresponds to <span class="math inline">\(\sigma_n(\cdot)\)</span> and the other arguments are as in EI.</p>
</section>
<section id="knowledge-gradient-1" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-gradient-1">Knowledge Gradient</h3>
<p><a href="#eq-updatemuKG" class="quarto-xref">Equation&nbsp;6</a> is mirrored in the code as:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nsim number of simulated observations from a normal</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(nsim, mu_i, sigma_i)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the covariance between every point in the grid and the point x_i</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>cov_xp_xi <span class="ot">&lt;-</span> cov_grid[, i]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># denominator term in the KG update formula</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>denom <span class="ot">&lt;-</span> var[i] <span class="sc">+</span> obs_noise_var</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># For each simulated y, compute updated max(mu)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>max_after <span class="ot">&lt;-</span> <span class="fu">numeric</span>(nsim)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">seq_len</span>(nsim)) {</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                <span class="co"># says what would the new maximum mu be if we observed this</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                <span class="co"># simulated value at the point x_i   </span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Evaluates the posterior mean for every point in the </span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                <span class="co"># space, updating other points based on closeness </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                <span class="co"># to the evaluated point by using the covariance matrix</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                mu_new <span class="ot">&lt;-</span> mu <span class="sc">+</span> cov_xp_xi <span class="sc">*</span> (y_sim[s] <span class="sc">-</span> mu_i) <span class="sc">/</span> denom</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                <span class="co"># takes this new maximum mu into a vector for later averaging</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                max_after[s] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu_new)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            }</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where <code>mu_new</code> is <span class="math inline">\(\mu_{n+1}(x)\)</span>, <code>cov_xp_xi</code> is <span class="math inline">\(\operatorname{cov}_n(x_{n+1},x)\)</span>, <code>y_sim[s]</code> is a simulated value drawn from <span class="math inline">\(F_{n+1}\)</span> and <code>mu_i</code> is <span class="math inline">\(\mu_n(x_{n+1})\)</span>. We can also see that <code>var[i]</code> is <span class="math inline">\(\operatorname{var}_n(x_{n+1})\)</span> and <code>obs_noise_var</code> is <span class="math inline">\(\sigma_{\mathrm{obs}}^2\)</span> which makes the denominators equivalent.</p>
<p>Now, we have everything needed to compute <a href="#eq-KnowledgeGradient" class="quarto-xref">Equation&nbsp;5</a> . We can write the whole process in code as below:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>knowledge_gradient_sim <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma, model, <span class="at">obs_noise_var =</span> <span class="dv">0</span>, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">nsim =</span> <span class="dv">100</span>, pred_risk, <span class="at">eps =</span> <span class="fl">1e-4</span>) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>{ </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    X_pred <span class="ot">&lt;-</span> dat[, <span class="fu">c</span>(<span class="st">"Ftrgt"</span>, <span class="st">"Btrigger"</span>)]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    cov_grid <span class="ot">&lt;-</span> <span class="fu">cov_exp</span>(X_pred, X_pred, <span class="at">theta =</span> model<span class="sc">@</span>covariance<span class="sc">@</span>range.val, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">sigma2 =</span> model<span class="sc">@</span>covariance<span class="sc">@</span>sd2)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">length</span>(mu)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    var <span class="ot">&lt;-</span> sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Current best mean catch</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    mu_best <span class="ot">&lt;-</span> <span class="fu">max</span>(mu)   </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    kg <span class="ot">&lt;-</span> <span class="fu">numeric</span>(m)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop over all candidate points</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_len</span>(m)) {</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Set KG to 0 for points where risk is too high</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (pred_risk[i] <span class="sc">&lt;</span> eps) {</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>                kg[i] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">next</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            mu_i <span class="ot">&lt;-</span> mu[i]</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            sigma_i <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(var[i] <span class="sc">+</span> obs_noise_var)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># nsim simulated observations</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            y_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(nsim, mu_i, sigma_i)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the covariance between every point in the grid and </span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the point x_i</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            cov_xp_xi <span class="ot">&lt;-</span> cov_grid[, i]</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># denominator term in the KG update formula</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            denom <span class="ot">&lt;-</span> var[i] <span class="sc">+</span> obs_noise_var</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># For each simulated y, compute updated max(mu)</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            max_after <span class="ot">&lt;-</span> <span class="fu">numeric</span>(nsim)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">seq_len</span>(nsim)) {</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                <span class="co"># says what would the new maximum mu be if we observed this</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                <span class="co"># simulated value at the point x_i by implementing </span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                <span class="co"># the standard formula for this in GP posterior updating   </span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This evaluates the posterior mean for every point in the </span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>                <span class="co"># space, updating other points based on closeness </span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># to the evaluated point by using the cov matrix</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>                mu_new <span class="ot">&lt;-</span> mu <span class="sc">+</span> cov_xp_xi <span class="sc">*</span> (y_sim[s] <span class="sc">-</span> mu_i) <span class="sc">/</span> denom</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>                <span class="co"># takes this new maximum mu into a vector for averaging</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>                max_after[s] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu_new)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Computes expected increase in the maximum posterior mean</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>            kg[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(max_after <span class="sc">-</span> mu_best)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns the vector of KG values for each point in the design space</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    kg</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where the KG for each point is calculated by:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>kg[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(max_after <span class="sc">-</span> mu_best)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>where <code>max_after</code> is <span class="math inline">\(\mu_{n+1}^*\)</span> and <code>mu_best</code> is <span class="math inline">\(\mu_n^*\)</span>.</p>
<p>All the arguments of the function are as in EI with a few exceptions. <code>obs_noise_var</code> is the new name for the noise variable set by the user; <code>theta</code> is the length scale of our GP determining how quickly the objective function changes as <span class="math inline">\(Ftarget\)</span> and <span class="math inline">\(Btrigger\)</span> change; and <code>nsim</code> tells the code how many times to estimate the value of <span class="math inline">\(f(x_{n+1})\)</span> using <span class="math inline">\(F(x_{n+1})\)</span> before we calculate <span class="math inline">\(KG_n(x)\)</span><span class="citation" data-cites="DiceKrigingDocumentation">(<a href="#ref-DiceKrigingDocumentation" role="doc-biblioref">Roustant 2025</a>)</span>.</p>
</section>
<section id="kmeans-process" class="level3">
<h3 class="anchored" data-anchor-id="kmeans-process">Kmeans process</h3>
<p>To then make sure that the next points we sample are spread out across the sample space, we use the Kmeans process as coded up below <span class="citation" data-cites="batchspreadingoutjustification">(<a href="#ref-batchspreadingoutjustification" role="doc-biblioref">Azimi, Fern, and Fern 2010</a>)</span>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select next points</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># cand is our Possible Space</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a> <span class="cf">if</span> (<span class="fu">nrow</span>(cand) <span class="sc">&lt;=</span> <span class="dv">8</span>) {</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Here, we take all of the points as there are less than the </span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># amount we want to sample left</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>   next_points <span class="ot">&lt;-</span> cand[<span class="fu">order</span>(<span class="sc">-</span>cand<span class="sc">$</span>ei), <span class="fu">c</span>(<span class="st">"Ftarget"</span>, <span class="st">"Btrigger"</span>)]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a> } <span class="cf">else</span> {</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We order the points based on the value given </span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># by the acquisition function, from highest to lowest here</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>   top_candidates <span class="ot">&lt;-</span> cand[<span class="fu">order</span>(<span class="sc">-</span>cand<span class="sc">$</span>ei), ][<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(cand), ]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>   <span class="co"># We need to set a seed to make the points that kmeans</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># picks as the first centroids reproducible</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>   <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>   <span class="co"># We find the clusters</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>   km_result <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(top_candidates[, <span class="fu">c</span>(<span class="st">"Ftarget"</span>, <span class="st">"Btrigger"</span>)], <span class="at">centers =</span> <span class="dv">8</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>   <span class="co"># We add the cluster column to our table</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>   top_candidates<span class="sc">$</span>cluster <span class="ot">&lt;-</span> km_result<span class="sc">$</span>cluster</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>   <span class="co"># We pick the point with the highest value from the </span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>   <span class="co"># acquisition function from each cluster and assemble these </span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>   <span class="co"># points into a vector</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>   next_points <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">lapply</span>(<span class="fu">split</span>(top_candidates, top_candidates<span class="sc">$</span>cluster),</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                         <span class="cf">function</span>(df) {</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>     df[<span class="fu">which.max</span>(df<span class="sc">$</span>ei), <span class="fu">c</span>(<span class="st">"Ftarget"</span>, <span class="st">"Btrigger"</span>, <span class="st">"ei"</span>)]</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>   }))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a> }</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then, at the beginning of our next round we sample the eight points given in <code>next_points</code>.</p>
</section>
</section>
<section id="updating-our-gps" class="level2">
<h2 class="anchored" data-anchor-id="updating-our-gps">Updating our GPS</h2>
<p>In our second round, we need to update our GPs with new data <span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Our process for updating GPs can be seen in the code below:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>gp_risk <span class="ot">&lt;-</span> <span class="fu">km</span>(<span class="sc">~</span>.<span class="sc">^</span><span class="dv">2</span>, <span class="at">design =</span> runs[, <span class="fu">c</span>(<span class="st">"Ftarget"</span>, <span class="st">"Btrigger"</span>)], <span class="at">estim.method =</span> <span class="st">"MLE"</span>,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">response =</span> res_risk, <span class="at">nugget =</span> <span class="fl">1e-12</span> <span class="sc">*</span> <span class="fu">var</span>(res_risk), <span class="at">covtype =</span> <span class="st">"exp"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This looks the same as before, but the <code>runs</code> variable includes all of our evaluated points and so we are adding the points that have been newly evaluated this round. Hence, we can do the calculations from the Kriging section to update the GP with a new mean vector and covariance kernel for our next round.</p>
<p>For the catch GP, we move on to using a new prior because by the time we create this GP we have sampled 16 points<span class="citation" data-cites="Originalpaper">(<a href="#ref-Originalpaper" role="doc-biblioref">Spence 2025</a>)</span>. Here is the catch GP for the second round and onwards:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>gp_cat <span class="ot">&lt;-</span> <span class="fu">km</span>(<span class="sc">~</span><span class="fu">I</span>(<span class="fu">log</span>(Ftarget<span class="fl">+0.1</span>)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(<span class="fu">log</span>(Ftarget<span class="fl">+0.1</span>))<span class="sc">+</span> <span class="fu">I</span>(<span class="fu">log</span>(Ftarget<span class="fl">+0.1</span>)<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                <span class="fu">I</span>(Btrigger) <span class="sc">+</span> <span class="fu">I</span>(Btrigger <span class="sc">*</span> <span class="fu">log</span>(Ftarget<span class="fl">+0.1</span>)),</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">design=</span>runs[,<span class="fu">c</span>(<span class="st">"Ftarget"</span>,<span class="st">"Btrigger"</span>)],<span class="at">estim.method=</span><span class="st">"MLE"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">response =</span> res_cat,<span class="at">nugget=</span><span class="fl">1e-12</span><span class="sc">*</span><span class="fu">var</span>(res_cat),<span class="at">covtype =</span> <span class="st">"exp"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now, we repeat the full process described in the Application of theory in my project section until there are no points with a positive KG. Then, the optimal point is the <span class="math inline">\(x^*\)</span> that has the highest catch out of the precautionary points.</p>
<section id="references" class="level3 unnumbered">


</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-batchspreadingoutjustification" class="csl-entry" role="listitem">
Azimi, Javad, Alan Fern, and Xiaoli Fern. 2010. <span>“Batch Bayesian Optimization via Simulation Matching.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta. Vol. 23. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf</a>.
</div>
<div id="ref-BayesianOptimisationTutorial" class="csl-entry" role="listitem">
Frazier, Peter. 2018. <span>“A Tutorial on Bayesian Optimization.”</span> <a href="https://doi.org/10.48550/arXiv.1807.02811">https://doi.org/10.48550/arXiv.1807.02811</a>.
</div>
<div id="ref-AKMeansClusteringAlgorithm" class="csl-entry" role="listitem">
Hartigan, J. A., and M. A. Wong. 2018. <span>“A k-Means Clustering Algorithm.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 28 (1): 100–108. <a href="https://doi.org/10.2307/2346830">https://doi.org/10.2307/2346830</a>.
</div>
<div id="ref-GlobalOptimizationofStochasticBlackBoxSystemsviaSequentialKrigingMetaModels" class="csl-entry" role="listitem">
Huang, D., Theodore Allen, William Notz, and Ning Zheng. 2006. <span>“Global Optimization of Stochastic BlackBox Systems via Sequential Kriging Meta-Models.”</span> <em>Journal of Global Optimization</em> 34: 441–66. <a href="https://doi.org/10.1007/s10898-005-2454-3">https://doi.org/10.1007/s10898-005-2454-3</a>.
</div>
<div id="ref-EfficientGlobalOptimizationofExpensiveBlackBoxFunctions" class="csl-entry" role="listitem">
Jones, Donald, Matthias Schonlau, and William Welch. 1998. <span>“Efficient Global Optimization of Expensive Black-Box Functions.”</span> <em>Journal of Global Optimization</em> 13: 455–92. <a href="https://doi.org/10.1023/A:1008306431147">https://doi.org/10.1023/A:1008306431147</a>.
</div>
<div id="ref-kodinariya2013review" class="csl-entry" role="listitem">
Kodinariya, Trupti M, Prashant R Makwana, et al. 2013. <span>“Review on Determining Number of Cluster in k-Means Clustering.”</span> <em>International Journal</em> 1 (6): 90–95.
</div>
<div id="ref-letham2018constrainedbayesianoptimizationnoisy" class="csl-entry" role="listitem">
Letham, Benjamin, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. 2018. <span>“Constrained Bayesian Optimization with Noisy Experiments.”</span> <a href="https://arxiv.org/abs/1706.07094">https://arxiv.org/abs/1706.07094</a>.
</div>
<div id="ref-kmeansdocumentation" class="csl-entry" role="listitem">
RDocumentation. 2025. <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans">https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans</a>.
</div>
<div id="ref-DiceKrigingDocumentation" class="csl-entry" role="listitem">
Roustant, Olivier. 2025. <a href="https://www.rdocumentation.org/packages/DiceKriging/versions/1.6.1">https://www.rdocumentation.org/packages/DiceKriging/versions/1.6.1</a>.
</div>
<div id="ref-Roustant2012DiceKrigingpaper" class="csl-entry" role="listitem">
Roustant, Olivier, David Ginsbourger, and Yves Deville. 2012. <span>“DiceKriging, DiceOptim: Two r Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization.”</span> <em>Journal of Statistical Software</em> 51 (1): 1–55. <a href="https://doi.org/10.18637/jss.v051.i01">https://doi.org/10.18637/jss.v051.i01</a>.
</div>
<div id="ref-Originalpaper" class="csl-entry" role="listitem">
Spence, Michael A. 2025. <span>“Using History Matching to Speed up Management Strategy Evaluation Grid Searches.”</span> <em>Canadian Journal of Fisheries and Aquatic Sciences</em> 82: 1–14. <a href="https://doi.org/10.1139/cjfas-2024-0191">https://doi.org/10.1139/cjfas-2024-0191</a>.
</div>
<div id="ref-ungredda2022efficientcomputationknowledgegradient" class="csl-entry" role="listitem">
Ungredda, Juan, Michael Pearce, and Juergen Branke. 2022. <span>“Efficient Computation of the Knowledge Gradient for Bayesian Optimization.”</span> <a href="https://arxiv.org/abs/2209.15367">https://arxiv.org/abs/2209.15367</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>